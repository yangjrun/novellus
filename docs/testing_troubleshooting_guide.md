# ğŸ”§ æµ‹è¯•ç³»ç»Ÿæ•…éšœæ’é™¤æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†é’ˆå¯¹H:\novellusé¡¹ç›®æµ‹è¯•éªŒè¯ç³»ç»Ÿçš„å…¨é¢æ•…éšœæ’é™¤æ–¹æ¡ˆï¼Œè¦†ç›–pgvectoræ‰©å±•ã€AIæ¨¡å‹ç®¡ç†ç³»ç»Ÿã€é›†æˆæµ‹è¯•å’Œæ€§èƒ½åŸºå‡†æµ‹è¯•çš„å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒæ£€æŸ¥å’Œé¢„å¤‡æ­¥éª¤](#ç¯å¢ƒæ£€æŸ¥å’Œé¢„å¤‡æ­¥éª¤)
2. [pgvectoræ‰©å±•æ•…éšœæ’é™¤](#pgvectoræ‰©å±•æ•…éšœæ’é™¤)
3. [AIæ¨¡å‹ç®¡ç†ç³»ç»Ÿæ•…éšœæ’é™¤](#aiæ¨¡å‹ç®¡ç†ç³»ç»Ÿæ•…éšœæ’é™¤)
4. [é›†æˆæµ‹è¯•æ•…éšœæ’é™¤](#é›†æˆæµ‹è¯•æ•…éšœæ’é™¤)
5. [æ€§èƒ½åŸºå‡†æµ‹è¯•æ•…éšœæ’é™¤](#æ€§èƒ½åŸºå‡†æµ‹è¯•æ•…éšœæ’é™¤)
6. [è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå™¨æ•…éšœæ’é™¤](#è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå™¨æ•…éšœæ’é™¤)
7. [æ•°æ®åº“è¿æ¥é—®é¢˜](#æ•°æ®åº“è¿æ¥é—®é¢˜)
8. [Redisç¼“å­˜é—®é¢˜](#redisç¼“å­˜é—®é¢˜)
9. [ç³»ç»Ÿèµ„æºé—®é¢˜](#ç³»ç»Ÿèµ„æºé—®é¢˜)
10. [æ—¥å¿—åˆ†æå’Œè¯Šæ–­](#æ—¥å¿—åˆ†æå’Œè¯Šæ–­)

---

## ğŸ” ç¯å¢ƒæ£€æŸ¥å’Œé¢„å¤‡æ­¥éª¤

### å¿«é€Ÿç¯å¢ƒéªŒè¯è„šæœ¬

```bash
#!/bin/bash
# ç¯å¢ƒéªŒè¯è„šæœ¬

echo "ğŸ” Novellusæµ‹è¯•ç¯å¢ƒéªŒè¯"
echo "========================"

# 1. æ£€æŸ¥Pythonç¯å¢ƒ
echo "æ£€æŸ¥Pythonç¯å¢ƒ..."
python3 --version
pip3 list | grep -E "(asyncpg|redis|psutil|numpy)"

# 2. æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo "æ£€æŸ¥PostgreSQLè¿æ¥..."
psql -h localhost -p 5432 -U postgres -d postgres -c "SELECT version();"

# 3. æ£€æŸ¥pgvectoræ‰©å±•
echo "æ£€æŸ¥pgvectoræ‰©å±•..."
psql -h localhost -p 5432 -U postgres -d postgres -c "SELECT name, installed_version FROM pg_available_extensions WHERE name = 'vector';"

# 4. æ£€æŸ¥Redisè¿æ¥
echo "æ£€æŸ¥Redisè¿æ¥..."
redis-cli ping

# 5. æ£€æŸ¥é¡¹ç›®ç»“æ„
echo "æ£€æŸ¥é¡¹ç›®ç»“æ„..."
ls -la /h/novellus/src/
ls -la /h/novellus/tests/

echo "âœ… ç¯å¢ƒéªŒè¯å®Œæˆ"
```

### åŸºç¡€ä¾èµ–æ£€æŸ¥

```python
#!/usr/bin/env python3
"""
ä¾èµ–æ£€æŸ¥è„šæœ¬
"""
import sys
import importlib

required_packages = [
    'asyncio',
    'asyncpg',
    'redis',
    'psutil',
    'numpy',
    'matplotlib',
    'seaborn',
    'jinja2',
    'pandas'
]

def check_dependencies():
    missing_packages = []

    for package in required_packages:
        try:
            importlib.import_module(package)
            print(f"âœ… {package}")
        except ImportError:
            print(f"âŒ {package} - æœªå®‰è£…")
            missing_packages.append(package)

    if missing_packages:
        print(f"\nç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œ: pip install " + " ".join(missing_packages))
        return False

    print("\nâœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    return True

if __name__ == "__main__":
    check_dependencies()
```

---

## ğŸ”§ pgvectoræ‰©å±•æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. pgvectoræ‰©å±•æœªå®‰è£…

**ç—‡çŠ¶:**
```
ERROR: extension "vector" is not available
```

**è¯Šæ–­:**
```sql
-- æ£€æŸ¥å¯ç”¨æ‰©å±•
SELECT name, installed_version, default_version
FROM pg_available_extensions
WHERE name = 'vector';

-- æ£€æŸ¥å·²å®‰è£…æ‰©å±•
SELECT extname, extversion
FROM pg_extension
WHERE extname = 'vector';
```

**è§£å†³æ–¹æ¡ˆ:**
```sql
-- å®‰è£…pgvectoræ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;

-- éªŒè¯å®‰è£…
SELECT vector_version();
```

**Dockerç¯å¢ƒè§£å†³æ–¹æ¡ˆ:**
```bash
# ä½¿ç”¨å®˜æ–¹pgvectoré•œåƒ
docker run -d \
  --name postgres-pgvector \
  -e POSTGRES_PASSWORD=postgres \
  -p 5432:5432 \
  pgvector/pgvector:pg15
```

#### 2. å‘é‡ç»´åº¦ä¸åŒ¹é…

**ç—‡çŠ¶:**
```
ERROR: expected 1536 dimensions, not 512
```

**è¯Šæ–­è„šæœ¬:**
```python
async def diagnose_vector_dimensions(db_pool):
    async with db_pool.acquire() as conn:
        # æ£€æŸ¥è¡¨ç»“æ„
        result = await conn.fetch("""
            SELECT column_name, data_type, character_maximum_length
            FROM information_schema.columns
            WHERE table_name = 'content_embeddings'
            AND column_name = 'embedding'
        """)

        # æ£€æŸ¥å®é™…æ•°æ®
        sample = await conn.fetch("""
            SELECT array_length(embedding::float[], 1) as dimensions
            FROM content_embeddings
            LIMIT 5
        """)

        return {
            'table_definition': result,
            'actual_dimensions': [row['dimensions'] for row in sample]
        }
```

**è§£å†³æ–¹æ¡ˆ:**
```sql
-- ä¿®æ”¹è¡¨ç»“æ„
ALTER TABLE content_embeddings
ALTER COLUMN embedding TYPE vector(1536);

-- æˆ–é‡å»ºè¡¨
DROP TABLE IF EXISTS content_embeddings CASCADE;
CREATE TABLE content_embeddings (
    content_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    embedding vector(1536),
    -- å…¶ä»–å­—æ®µ...
);
```

#### 3. å‘é‡ç´¢å¼•æ€§èƒ½é—®é¢˜

**ç—‡çŠ¶:**
- æŸ¥è¯¢é€Ÿåº¦æ…¢
- ç´¢å¼•åˆ›å»ºå¤±è´¥

**è¯Šæ–­:**
```sql
-- æ£€æŸ¥ç´¢å¼•çŠ¶æ€
SELECT schemaname, tablename, indexname, indexdef
FROM pg_indexes
WHERE tablename LIKE '%vector%';

-- æ£€æŸ¥è¡¨å¤§å°
SELECT pg_size_pretty(pg_total_relation_size('content_embeddings'));

-- æ£€æŸ¥æŸ¥è¯¢è®¡åˆ’
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM content_embeddings
ORDER BY embedding <-> '[0,1,0,...]'::vector
LIMIT 10;
```

**è§£å†³æ–¹æ¡ˆ:**
```sql
-- ä¼˜åŒ–HNSWç´¢å¼•
DROP INDEX IF EXISTS idx_embedding_hnsw;
CREATE INDEX CONCURRENTLY idx_embedding_hnsw
ON content_embeddings
USING hnsw (embedding vector_l2_ops)
WITH (m = 16, ef_construction = 64);

-- æˆ–ä¼˜åŒ–IVFFlatç´¢å¼•
DROP INDEX IF EXISTS idx_embedding_ivfflat;
CREATE INDEX CONCURRENTLY idx_embedding_ivfflat
ON content_embeddings
USING ivfflat (embedding vector_l2_ops)
WITH (lists = 100);

-- è°ƒæ•´æŸ¥è¯¢å‚æ•°
SET ivfflat.probes = 10;
SET hnsw.ef_search = 40;
```

### pgvectoræµ‹è¯•éªŒè¯è„šæœ¬

```python
#!/usr/bin/env python3
"""
pgvectoråŠŸèƒ½éªŒè¯è„šæœ¬
"""
import asyncio
import asyncpg
import random

async def verify_pgvector_functionality():
    """éªŒè¯pgvectoråŸºç¡€åŠŸèƒ½"""

    # è¿æ¥æ•°æ®åº“
    conn = await asyncpg.connect(
        host="localhost",
        port=5432,
        database="postgres",
        user="postgres",
        password="postgres"
    )

    try:
        print("ğŸ” éªŒè¯pgvectoråŠŸèƒ½...")

        # 1. æ£€æŸ¥æ‰©å±•
        version = await conn.fetchval("SELECT vector_version()")
        print(f"âœ… pgvectorç‰ˆæœ¬: {version}")

        # 2. æµ‹è¯•å‘é‡æ“ä½œ
        test_vector = [random.random() for _ in range(1536)]

        # åˆ›å»ºæµ‹è¯•è¡¨
        await conn.execute("""
            CREATE TEMP TABLE test_vectors (
                id SERIAL PRIMARY KEY,
                embedding vector(1536)
            )
        """)

        # æ’å…¥æµ‹è¯•æ•°æ®
        await conn.execute(
            "INSERT INTO test_vectors (embedding) VALUES ($1)",
            test_vector
        )

        # æµ‹è¯•ç›¸ä¼¼åº¦æŸ¥è¯¢
        result = await conn.fetchval("""
            SELECT embedding <-> $1 as distance
            FROM test_vectors
            ORDER BY embedding <-> $1
            LIMIT 1
        """, test_vector)

        print(f"âœ… å‘é‡è·ç¦»è®¡ç®—: {result}")

        # 3. æµ‹è¯•ä¸åŒè·ç¦»å‡½æ•°
        l2_dist = await conn.fetchval(
            "SELECT $1 <-> $2", test_vector, test_vector
        )
        cosine_dist = await conn.fetchval(
            "SELECT $1 <=> $2", test_vector, test_vector
        )
        inner_product = await conn.fetchval(
            "SELECT $1 <#> $2", test_vector, test_vector
        )

        print(f"âœ… L2è·ç¦»: {l2_dist}")
        print(f"âœ… ä½™å¼¦è·ç¦»: {cosine_dist}")
        print(f"âœ… å†…ç§¯: {inner_product}")

        return True

    except Exception as e:
        print(f"âŒ pgvectoréªŒè¯å¤±è´¥: {e}")
        return False

    finally:
        await conn.close()

if __name__ == "__main__":
    asyncio.run(verify_pgvector_functionality())
```

---

## ğŸ¤– AIæ¨¡å‹ç®¡ç†ç³»ç»Ÿæ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. æ¨¡å‹APIè¿æ¥å¤±è´¥

**ç—‡çŠ¶:**
```
ConnectionError: Failed to connect to OpenAI API
AuthenticationError: Invalid API key
```

**è¯Šæ–­è„šæœ¬:**
```python
async def diagnose_ai_model_connections():
    """è¯Šæ–­AIæ¨¡å‹è¿æ¥é—®é¢˜"""

    diagnostics = {
        'api_keys_configured': False,
        'network_connectivity': False,
        'model_availability': {},
        'rate_limits': {}
    }

    # æ£€æŸ¥APIå¯†é’¥é…ç½®
    import os
    api_keys = {
        'openai': os.getenv('OPENAI_API_KEY'),
        'anthropic': os.getenv('CLAUDE_API_KEY')
    }

    diagnostics['api_keys_configured'] = any(api_keys.values())

    # æµ‹è¯•ç½‘ç»œè¿æ¥
    try:
        import aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.get('https://api.openai.com/v1/models') as resp:
                diagnostics['network_connectivity'] = resp.status == 401  # 401è¡¨ç¤ºè®¤è¯é—®é¢˜ï¼Œä½†ç½‘ç»œé€š
    except:
        diagnostics['network_connectivity'] = False

    return diagnostics
```

**è§£å†³æ–¹æ¡ˆ:**
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="your-openai-api-key"
export CLAUDE_API_KEY="your-claude-api-key"

# æˆ–åœ¨.envæ–‡ä»¶ä¸­é…ç½®
echo "OPENAI_API_KEY=your-key" >> .env
echo "CLAUDE_API_KEY=your-key" >> .env
```

#### 2. æ¨¡å‹è´Ÿè½½å‡è¡¡é—®é¢˜

**ç—‡çŠ¶:**
- è¯·æ±‚æ€»æ˜¯è·¯ç”±åˆ°åŒä¸€ä¸ªæ¨¡å‹
- æŸäº›æ¨¡å‹ä»æœªè¢«é€‰æ‹©

**è¯Šæ–­:**
```python
async def diagnose_load_balancing(model_manager):
    """è¯Šæ–­è´Ÿè½½å‡è¡¡é—®é¢˜"""

    # æ¨¡æ‹Ÿå¤šä¸ªè¯·æ±‚
    model_selections = []
    for _ in range(50):
        selected = await model_manager.load_balancer.select_model(
            request_type="completion",
            prompt_length=100
        )
        model_selections.append(selected)

    # åˆ†æåˆ†å¸ƒ
    from collections import Counter
    distribution = Counter(model_selections)

    return {
        'total_requests': len(model_selections),
        'model_distribution': dict(distribution),
        'unique_models_used': len(distribution),
        'balance_score': 1 - max(distribution.values()) / len(model_selections)
    }
```

**è§£å†³æ–¹æ¡ˆ:**
```python
# è°ƒæ•´æ¨¡å‹ä¼˜å…ˆçº§
model_config.priority = 100  # æ›´é«˜ä¼˜å…ˆçº§

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
for model_id, model in model_manager.models.items():
    if model.status != ModelStatus.ACTIVE:
        print(f"æ¨¡å‹ {model_id} çŠ¶æ€: {model.status}")

# é‡ç½®é€Ÿç‡é™åˆ¶
model_manager.rate_limits.clear()
```

#### 3. ç¼“å­˜å‘½ä¸­ç‡ä½

**ç—‡çŠ¶:**
- ç¼“å­˜å‘½ä¸­ç‡ä½äºé¢„æœŸ
- å“åº”æ—¶é—´æ²¡æœ‰æ˜æ˜¾æ”¹å–„

**è¯Šæ–­:**
```python
async def diagnose_cache_performance(cache_manager):
    """è¯Šæ–­ç¼“å­˜æ€§èƒ½é—®é¢˜"""

    # æ£€æŸ¥ç¼“å­˜é…ç½®
    cache_stats = await cache_manager.get_cache_statistics()

    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    test_key = "test_cache_key"
    test_value = {"test": "data"}

    # å†™å…¥æµ‹è¯•
    await cache_manager.set_cache(test_key, test_value, ttl=60)

    # è¯»å–æµ‹è¯•
    cached_value = await cache_manager.get_cache(test_key)

    return {
        'cache_stats': cache_stats,
        'write_test_success': True,
        'read_test_success': cached_value == test_value,
        'cache_backend': type(cache_manager.redis_client).__name__
    }
```

**è§£å†³æ–¹æ¡ˆ:**
```python
# è°ƒæ•´ç¼“å­˜TTL
await cache_manager.set_cache(key, value, ttl=3600)  # 1å°æ—¶

# ä¼˜åŒ–ç¼“å­˜é”®ç”Ÿæˆ
def generate_cache_key(prompt, model_id, **kwargs):
    import hashlib
    cache_data = f"{prompt}_{model_id}_{sorted(kwargs.items())}"
    return hashlib.md5(cache_data.encode()).hexdigest()

# é¢„çƒ­ç¼“å­˜
common_prompts = ["å¸¸è§é—®é¢˜1", "å¸¸è§é—®é¢˜2", ...]
for prompt in common_prompts:
    response = await model_manager.complete(prompt, use_cache=True)
```

### AIæ¨¡å‹ç³»ç»Ÿç›‘æ§è„šæœ¬

```python
#!/usr/bin/env python3
"""
AIæ¨¡å‹ç³»ç»Ÿç›‘æ§è„šæœ¬
"""
import asyncio
import time
from datetime import datetime, timedelta

async def monitor_ai_model_system(model_manager, duration_minutes=5):
    """ç›‘æ§AIæ¨¡å‹ç³»ç»Ÿå¥åº·çŠ¶å†µ"""

    print(f"ğŸ” å¼€å§‹ç›‘æ§AIæ¨¡å‹ç³»ç»Ÿ ({duration_minutes}åˆ†é’Ÿ)")

    start_time = time.time()
    end_time = start_time + (duration_minutes * 60)

    metrics = {
        'requests_sent': 0,
        'requests_successful': 0,
        'cache_hits': 0,
        'model_usage': {},
        'error_count': 0,
        'avg_latency': 0,
        'latencies': []
    }

    while time.time() < end_time:
        try:
            # å‘é€æµ‹è¯•è¯·æ±‚
            request_start = time.time()

            response = await model_manager.complete(
                prompt=f"æµ‹è¯•è¯·æ±‚ {datetime.now().isoformat()}",
                use_cache=True
            )

            request_latency = (time.time() - request_start) * 1000

            # æ›´æ–°æŒ‡æ ‡
            metrics['requests_sent'] += 1
            metrics['latencies'].append(request_latency)

            if response and 'content' in response:
                metrics['requests_successful'] += 1

                # è®°å½•æ¨¡å‹ä½¿ç”¨
                model = response.get('model', 'unknown')
                metrics['model_usage'][model] = metrics['model_usage'].get(model, 0) + 1

                # æ£€æŸ¥æ˜¯å¦ç¼“å­˜å‘½ä¸­
                if response.get('cache_hit'):
                    metrics['cache_hits'] += 1

        except Exception as e:
            metrics['error_count'] += 1
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")

        # ç­‰å¾…é—´éš”
        await asyncio.sleep(10)

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    if metrics['latencies']:
        metrics['avg_latency'] = sum(metrics['latencies']) / len(metrics['latencies'])

    # è¾“å‡ºç›‘æ§ç»“æœ
    print("\nğŸ“Š ç›‘æ§ç»“æœ:")
    print(f"æ€»è¯·æ±‚æ•°: {metrics['requests_sent']}")
    print(f"æˆåŠŸç‡: {metrics['requests_successful'] / metrics['requests_sent']:.2%}")
    print(f"ç¼“å­˜å‘½ä¸­ç‡: {metrics['cache_hits'] / metrics['requests_sent']:.2%}")
    print(f"å¹³å‡å»¶è¿Ÿ: {metrics['avg_latency']:.2f}ms")
    print(f"é”™è¯¯è®¡æ•°: {metrics['error_count']}")
    print(f"æ¨¡å‹ä½¿ç”¨åˆ†å¸ƒ: {metrics['model_usage']}")

    return metrics
```

---

## ğŸ”— é›†æˆæµ‹è¯•æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. ç»„ä»¶é—´é€šä¿¡å¤±è´¥

**ç—‡çŠ¶:**
- é›†æˆæµ‹è¯•è¶…æ—¶
- æ•°æ®åœ¨ç»„ä»¶é—´ä¼ é€’æ—¶ä¸¢å¤±

**è¯Šæ–­è„šæœ¬:**
```python
async def diagnose_component_integration():
    """è¯Šæ–­ç»„ä»¶é›†æˆé—®é¢˜"""

    diagnostics = {
        'database_connectivity': False,
        'redis_connectivity': False,
        'ai_model_availability': False,
        'vector_search_functional': False,
        'cache_integration': False
    }

    try:
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        import asyncpg
        conn = await asyncpg.connect("postgresql://...")
        await conn.fetchval("SELECT 1")
        diagnostics['database_connectivity'] = True
        await conn.close()
    except:
        pass

    try:
        # æµ‹è¯•Redisè¿æ¥
        import redis.asyncio as redis
        r = await redis.from_url("redis://localhost:6379")
        await r.ping()
        diagnostics['redis_connectivity'] = True
        await r.close()
    except:
        pass

    # ç»§ç»­å…¶ä»–ç»„ä»¶æµ‹è¯•...

    return diagnostics
```

#### 2. æ•°æ®ä¸€è‡´æ€§é—®é¢˜

**ç—‡çŠ¶:**
- ä¸åŒç»„ä»¶è¿”å›ä¸ä¸€è‡´çš„æ•°æ®
- ç¼“å­˜æ•°æ®ä¸æ•°æ®åº“æ•°æ®ä¸åŒ¹é…

**éªŒè¯è„šæœ¬:**
```python
async def verify_data_consistency(integration_system):
    """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""

    test_content = "æµ‹è¯•æ•°æ®ä¸€è‡´æ€§çš„å†…å®¹"
    content_id = None

    try:
        # 1. é€šè¿‡é›†æˆç³»ç»Ÿå­˜å‚¨å†…å®¹
        store_result = await integration_system.store_content_with_embedding(
            content=test_content,
            content_type="consistency_test"
        )
        content_id = store_result['content_id']

        # 2. ç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢
        async with integration_system.db_pool.acquire() as conn:
            db_result = await conn.fetchrow(
                "SELECT * FROM content_embeddings WHERE content_id = $1",
                content_id
            )

        # 3. é€šè¿‡æœç´¢æŸ¥æ‰¾
        search_result = await integration_system.semantic_search(
            query_text=test_content,
            content_type="consistency_test"
        )

        # 4. æ£€æŸ¥ç¼“å­˜
        cache_result = await integration_system.cache_manager.get_cache(
            f"content_{content_id}"
        )

        # éªŒè¯ä¸€è‡´æ€§
        checks = {
            'database_has_content': db_result is not None,
            'search_finds_content': any(
                r.get('content_id') == content_id
                for r in search_result.get('results', [])
            ),
            'content_text_matches': db_result and db_result['content_text'] == test_content,
            'cache_consistency': cache_result is not None
        }

        return checks

    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        if content_id:
            async with integration_system.db_pool.acquire() as conn:
                await conn.execute(
                    "DELETE FROM content_embeddings WHERE content_id = $1",
                    content_id
                )
```

#### 3. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

**æ€§èƒ½è¯Šæ–­è„šæœ¬:**
```python
import time
import asyncio
from contextlib import asynccontextmanager

@asynccontextmanager
async def performance_monitor(operation_name):
    """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss

    print(f"â±ï¸ å¼€å§‹ {operation_name}")

    try:
        yield
    finally:
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss

        duration = (end_time - start_time) * 1000
        memory_delta = (end_memory - start_memory) / 1024 / 1024

        print(f"âœ… {operation_name} å®Œæˆ: {duration:.2f}ms, å†…å­˜å˜åŒ–: {memory_delta:.2f}MB")

async def profile_integration_performance(integration_system):
    """æ€§èƒ½åˆ†æé›†æˆç³»ç»Ÿ"""

    test_data = "è¿™æ˜¯ç”¨äºæ€§èƒ½æµ‹è¯•çš„å†…å®¹" * 100

    # æµ‹è¯•å‘é‡ç”Ÿæˆæ€§èƒ½
    async with performance_monitor("å‘é‡ç”Ÿæˆ"):
        embedding_result = await integration_system.generate_embedding(test_data)

    # æµ‹è¯•å­˜å‚¨æ€§èƒ½
    async with performance_monitor("æ•°æ®å­˜å‚¨"):
        store_result = await integration_system.store_content_with_embedding(
            content=test_data,
            content_type="performance_test"
        )

    # æµ‹è¯•æœç´¢æ€§èƒ½
    async with performance_monitor("è¯­ä¹‰æœç´¢"):
        search_result = await integration_system.semantic_search(
            query_text=test_data[:100],
            content_type="performance_test"
        )

    # æµ‹è¯•ç¼“å­˜æ€§èƒ½
    async with performance_monitor("ç¼“å­˜æ“ä½œ"):
        await integration_system.cache_manager.set_cache("perf_test", test_data)
        cached = await integration_system.cache_manager.get_cache("perf_test")

    return {
        'embedding_latency': embedding_result.get('latency_ms', 0),
        'storage_success': store_result.get('success', False),
        'search_results_count': len(search_result.get('results', [])),
        'cache_hit': cached == test_data
    }
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. åŸºå‡†æµ‹è¯•æ•°æ®ç”Ÿæˆç¼“æ…¢

**ç—‡çŠ¶:**
- æµ‹è¯•æ•°æ®æ’å…¥é€Ÿåº¦æ…¢
- å†…å­˜ä½¿ç”¨è¿‡é«˜

**ä¼˜åŒ–æ–¹æ¡ˆ:**
```python
async def optimized_batch_insert(db_pool, vectors, batch_size=1000):
    """ä¼˜åŒ–çš„æ‰¹é‡æ’å…¥"""

    async with db_pool.acquire() as conn:
        # ä½¿ç”¨äº‹åŠ¡å’Œæ‰¹é‡æ’å…¥
        async with conn.transaction():
            # å‡†å¤‡è¯­å¥
            stmt = await conn.prepare("""
                INSERT INTO benchmark_vectors (content_text, embedding, metadata)
                VALUES ($1, $2, $3)
            """)

            # æ‰¹é‡æ‰§è¡Œ
            batch = []
            for i, (content, embedding, metadata) in enumerate(vectors):
                batch.append((content, embedding, json.dumps(metadata)))

                if len(batch) >= batch_size:
                    await stmt.executemany(batch)
                    batch = []

                    # å‘¨æœŸæ€§æäº¤ä»¥é‡Šæ”¾å†…å­˜
                    if (i + 1) % (batch_size * 10) == 0:
                        print(f"å·²æ’å…¥ {i + 1} æ¡è®°å½•")

            # æ’å…¥å‰©ä½™æ•°æ®
            if batch:
                await stmt.executemany(batch)
```

#### 2. æ€§èƒ½æµ‹è¯•ç»“æœä¸ç¨³å®š

**ç—‡çŠ¶:**
- ç›¸åŒæµ‹è¯•å¤šæ¬¡è¿è¡Œç»“æœå·®å¼‚å¾ˆå¤§
- æ€§èƒ½æŒ‡æ ‡å‡ºç°å¼‚å¸¸å€¼

**ç¨³å®šæ€§æ”¹è¿›:**
```python
import statistics
import time

async def stable_performance_test(test_func, iterations=5, warmup=2):
    """ç¨³å®šçš„æ€§èƒ½æµ‹è¯•"""

    print(f"ğŸ”¥ é¢„çƒ­æµ‹è¯• ({warmup}æ¬¡)")
    for i in range(warmup):
        await test_func()

    print(f"ğŸ“Š æ­£å¼æµ‹è¯• ({iterations}æ¬¡)")
    latencies = []

    for i in range(iterations):
        start_time = time.time()
        await test_func()
        latency = (time.time() - start_time) * 1000
        latencies.append(latency)
        print(f"  ç¬¬{i+1}æ¬¡: {latency:.2f}ms")

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    results = {
        'iterations': iterations,
        'min_latency': min(latencies),
        'max_latency': max(latencies),
        'avg_latency': statistics.mean(latencies),
        'median_latency': statistics.median(latencies),
        'std_deviation': statistics.stdev(latencies) if len(latencies) > 1 else 0,
        'coefficient_of_variation': statistics.stdev(latencies) / statistics.mean(latencies) if len(latencies) > 1 and statistics.mean(latencies) > 0 else 0
    }

    # æ£€æŸ¥ç¨³å®šæ€§
    cv_threshold = 0.2  # å˜å¼‚ç³»æ•°é˜ˆå€¼
    results['is_stable'] = results['coefficient_of_variation'] < cv_threshold

    return results
```

#### 3. ç³»ç»Ÿèµ„æºç›‘æ§å¼‚å¸¸

**èµ„æºç›‘æ§æ”¹è¿›:**
```python
import psutil
import asyncio
from collections import deque

class SystemResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self, max_history=100):
        self.max_history = max_history
        self.cpu_history = deque(maxlen=max_history)
        self.memory_history = deque(maxlen=max_history)
        self.disk_io_history = deque(maxlen=max_history)
        self.network_io_history = deque(maxlen=max_history)
        self.monitoring = False

    async def start_monitoring(self, interval=1):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True

        while self.monitoring:
            try:
                # CPUä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=None)
                self.cpu_history.append(cpu_percent)

                # å†…å­˜ä½¿ç”¨
                memory = psutil.virtual_memory()
                self.memory_history.append(memory.percent)

                # ç£ç›˜IO
                disk_io = psutil.disk_io_counters()
                if disk_io:
                    self.disk_io_history.append({
                        'read_bytes': disk_io.read_bytes,
                        'write_bytes': disk_io.write_bytes
                    })

                # ç½‘ç»œIO
                network_io = psutil.net_io_counters()
                if network_io:
                    self.network_io_history.append({
                        'bytes_sent': network_io.bytes_sent,
                        'bytes_recv': network_io.bytes_recv
                    })

                await asyncio.sleep(interval)

            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(interval)

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False

    def get_resource_summary(self):
        """è·å–èµ„æºä½¿ç”¨æ‘˜è¦"""
        if not self.cpu_history:
            return None

        return {
            'cpu': {
                'avg': statistics.mean(self.cpu_history),
                'max': max(self.cpu_history),
                'current': self.cpu_history[-1] if self.cpu_history else 0
            },
            'memory': {
                'avg': statistics.mean(self.memory_history),
                'max': max(self.memory_history),
                'current': self.memory_history[-1] if self.memory_history else 0
            },
            'monitoring_duration': len(self.cpu_history),
            'resource_alerts': self._check_resource_alerts()
        }

    def _check_resource_alerts(self):
        """æ£€æŸ¥èµ„æºè­¦æŠ¥"""
        alerts = []

        if self.cpu_history:
            avg_cpu = statistics.mean(self.cpu_history)
            if avg_cpu > 90:
                alerts.append("CPUä½¿ç”¨ç‡è¿‡é«˜")

        if self.memory_history:
            avg_memory = statistics.mean(self.memory_history)
            if avg_memory > 90:
                alerts.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")

        return alerts
```

---

## ğŸ¤– è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå™¨æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. æµ‹è¯•æ‰§è¡Œå™¨æ— æ³•å¯åŠ¨

**ç—‡çŠ¶:**
```
ModuleNotFoundError: No module named 'test_pgvector_suite'
ImportError: cannot import name 'AIModelManager'
```

**è¯Šæ–­è„šæœ¬:**
```python
#!/usr/bin/env python3
"""
æµ‹è¯•æ‰§è¡Œå™¨è¯Šæ–­è„šæœ¬
"""
import sys
import os
from pathlib import Path

def diagnose_test_runner():
    """è¯Šæ–­æµ‹è¯•æ‰§è¡Œå™¨é—®é¢˜"""

    print("ğŸ” è¯Šæ–­æµ‹è¯•æ‰§è¡Œå™¨ç¯å¢ƒ")

    # æ£€æŸ¥Pythonè·¯å¾„
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"Pythonè·¯å¾„: {sys.executable}")

    # æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•
    print(f"å½“å‰ç›®å½•: {os.getcwd()}")

    # æ£€æŸ¥é¡¹ç›®è·¯å¾„
    project_root = Path("/h/novellus")
    if not project_root.exists():
        print(f"âŒ é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨: {project_root}")
        return False

    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶
    test_files = [
        "tests/test_pgvector_suite.py",
        "tests/test_ai_model_manager.py",
        "tests/test_integration_suite.py",
        "tests/performance_benchmark_suite.py"
    ]

    for test_file in test_files:
        file_path = project_root / test_file
        if file_path.exists():
            print(f"âœ… {test_file}")
        else:
            print(f"âŒ {test_file}")

    # æ£€æŸ¥æºç ç›®å½•
    src_path = project_root / "src"
    if src_path.exists():
        print(f"âœ… srcç›®å½•å­˜åœ¨")

        # æ£€æŸ¥å…³é”®æ¨¡å—
        key_modules = [
            "src/ai/model_manager.py",
            "src/ai/cache_manager.py",
            "src/config.py"
        ]

        for module in key_modules:
            module_path = project_root / module
            if module_path.exists():
                print(f"âœ… {module}")
            else:
                print(f"âŒ {module}")
    else:
        print(f"âŒ srcç›®å½•ä¸å­˜åœ¨")

    # æ£€æŸ¥Pythonè·¯å¾„é…ç½®
    print(f"\nPythonæœç´¢è·¯å¾„:")
    for path in sys.path:
        print(f"  {path}")

    return True

if __name__ == "__main__":
    diagnose_test_runner()
```

**è§£å†³æ–¹æ¡ˆ:**
```bash
# è®¾ç½®Pythonè·¯å¾„
export PYTHONPATH="/h/novellus:/h/novellus/src:/h/novellus/tests:$PYTHONPATH"

# æˆ–åœ¨è„šæœ¬ä¸­æ·»åŠ 
import sys
sys.path.append('/h/novellus')
sys.path.append('/h/novellus/src')
sys.path.append('/h/novellus/tests')
```

#### 2. å¹¶è¡Œæµ‹è¯•å†²çª

**ç—‡çŠ¶:**
- å¹¶è¡Œæµ‹è¯•æ—¶æ•°æ®åº“è¿æ¥æ± è€—å°½
- æµ‹è¯•ç»“æœä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ:**
```python
# ä¼˜åŒ–è¿æ¥æ± é…ç½®
async def create_optimized_pool():
    """åˆ›å»ºä¼˜åŒ–çš„è¿æ¥æ± """

    # æ ¹æ®å¹¶è¡Œæµ‹è¯•æ•°é‡è°ƒæ•´è¿æ¥æ± å¤§å°
    import os
    concurrent_tests = int(os.getenv('CONCURRENT_TESTS', '4'))

    pool = await asyncpg.create_pool(
        host="localhost",
        port=5432,
        database="postgres",
        user="postgres",
        password="postgres",
        min_size=concurrent_tests,
        max_size=concurrent_tests * 5,
        command_timeout=60,
        server_settings={
            'jit': 'off',  # ç¦ç”¨JITä»¥æé«˜ç¨³å®šæ€§
            'application_name': 'novellus_tests'
        }
    )

    return pool

# æµ‹è¯•æ•°æ®éš”ç¦»
class TestDataIsolation:
    """æµ‹è¯•æ•°æ®éš”ç¦»ç®¡ç†å™¨"""

    def __init__(self):
        self.test_schemas = set()

    async def create_test_schema(self, pool, test_name):
        """ä¸ºæµ‹è¯•åˆ›å»ºç‹¬ç«‹çš„æ¨¡å¼"""
        schema_name = f"test_{test_name}_{int(time.time())}"

        async with pool.acquire() as conn:
            await conn.execute(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")
            await conn.execute(f"SET search_path = {schema_name}, public")

            # å¤åˆ¶å¿…è¦çš„è¡¨ç»“æ„
            tables_to_copy = [
                'content_embeddings',
                'ai_models',
                'ai_requests'
            ]

            for table in tables_to_copy:
                await conn.execute(f"""
                    CREATE TABLE {schema_name}.{table}
                    (LIKE public.{table} INCLUDING ALL)
                """)

        self.test_schemas.add(schema_name)
        return schema_name

    async def cleanup_test_schemas(self, pool):
        """æ¸…ç†æµ‹è¯•æ¨¡å¼"""
        async with pool.acquire() as conn:
            for schema in self.test_schemas:
                await conn.execute(f"DROP SCHEMA IF EXISTS {schema} CASCADE")

        self.test_schemas.clear()
```

#### 3. æŠ¥å‘Šç”Ÿæˆå¤±è´¥

**ç—‡çŠ¶:**
- HTMLæŠ¥å‘Šæ¨¡æ¿é”™è¯¯
- å›¾è¡¨ç”Ÿæˆå¤±è´¥

**è§£å†³æ–¹æ¡ˆ:**
```python
# ä¿®å¤æŠ¥å‘Šç”Ÿæˆé—®é¢˜
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

async def safe_report_generation(test_summary, output_dir):
    """å®‰å…¨çš„æŠ¥å‘Šç”Ÿæˆ"""

    try:
        # JSONæŠ¥å‘Šï¼ˆæœ€åŸºç¡€ï¼‰
        json_file = output_dir / "test_report.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(test_summary.__dict__, f, indent=2, default=str)
        print(f"âœ… JSONæŠ¥å‘Šç”ŸæˆæˆåŠŸ: {json_file}")

    except Exception as e:
        print(f"âŒ JSONæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

    try:
        # ç®€åŒ–HTMLæŠ¥å‘Š
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head><title>Test Report</title></head>
        <body>
            <h1>æµ‹è¯•æŠ¥å‘Š</h1>
            <p>è¿è¡ŒID: {test_summary.run_id}</p>
            <p>æ€»ä½“çŠ¶æ€: {'PASSED' if test_summary.overall_success else 'FAILED'}</p>
            <p>æˆåŠŸç‡: {test_summary.overall_success_rate:.2%}</p>
            <p>æ€»æµ‹è¯•æ•°: {test_summary.total_tests}</p>
            <p>é€šè¿‡: {test_summary.total_passed}</p>
            <p>å¤±è´¥: {test_summary.total_failed}</p>
        </body>
        </html>
        """

        html_file = output_dir / "test_report.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"âœ… HTMLæŠ¥å‘Šç”ŸæˆæˆåŠŸ: {html_file}")

    except Exception as e:
        print(f"âŒ HTMLæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
```

---

## ğŸ—„ï¸ æ•°æ®åº“è¿æ¥é—®é¢˜

### PostgreSQLè¿æ¥æ•…éšœæ’é™¤

#### è¿æ¥è¯Šæ–­è„šæœ¬
```python
#!/usr/bin/env python3
"""
PostgreSQLè¿æ¥è¯Šæ–­è„šæœ¬
"""
import asyncio
import asyncpg
import psutil
import time

async def diagnose_postgresql_connection():
    """è¯Šæ–­PostgreSQLè¿æ¥é—®é¢˜"""

    connection_configs = [
        {
            'name': 'æœ¬åœ°é»˜è®¤é…ç½®',
            'config': {
                'host': 'localhost',
                'port': 5432,
                'database': 'postgres',
                'user': 'postgres',
                'password': 'postgres'
            }
        },
        {
            'name': 'Dockeré…ç½®',
            'config': {
                'host': '127.0.0.1',
                'port': 5432,
                'database': 'postgres',
                'user': 'postgres',
                'password': 'postgres'
            }
        }
    ]

    for config_info in connection_configs:
        print(f"\nğŸ” æµ‹è¯• {config_info['name']}")

        try:
            # æµ‹è¯•è¿æ¥
            conn = await asyncpg.connect(**config_info['config'])

            # åŸºç¡€æŸ¥è¯¢æµ‹è¯•
            version = await conn.fetchval("SELECT version()")
            print(f"âœ… è¿æ¥æˆåŠŸ")
            print(f"   PostgreSQLç‰ˆæœ¬: {version.split(',')[0]}")

            # æµ‹è¯•pgvector
            try:
                vector_version = await conn.fetchval("SELECT vector_version()")
                print(f"âœ… pgvectorå¯ç”¨: {vector_version}")
            except:
                print(f"âŒ pgvectorä¸å¯ç”¨")

            # æµ‹è¯•æ€§èƒ½
            start_time = time.time()
            await conn.fetchval("SELECT pg_sleep(0.001)")
            latency = (time.time() - start_time) * 1000
            print(f"âœ… è¿æ¥å»¶è¿Ÿ: {latency:.2f}ms")

            # æ£€æŸ¥è¿æ¥æ± 
            pool_info = await conn.fetch("""
                SELECT
                    state,
                    COUNT(*) as count
                FROM pg_stat_activity
                WHERE datname = current_database()
                GROUP BY state
            """)
            print(f"âœ… æ´»åŠ¨è¿æ¥: {dict(pool_info)}")

            await conn.close()

        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")

            # æä¾›è¯Šæ–­å»ºè®®
            if "Connection refused" in str(e):
                print("   å»ºè®®: æ£€æŸ¥PostgreSQLæ˜¯å¦è¿è¡Œï¼Œç«¯å£æ˜¯å¦æ­£ç¡®")
            elif "authentication failed" in str(e):
                print("   å»ºè®®: æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç ï¼ŒæŸ¥çœ‹pg_hba.confé…ç½®")
            elif "database does not exist" in str(e):
                print("   å»ºè®®: åˆ›å»ºæ•°æ®åº“æˆ–ä½¿ç”¨æ­£ç¡®çš„æ•°æ®åº“å")

# è¿æ¥æ± è¯Šæ–­
async def diagnose_connection_pool():
    """è¯Šæ–­è¿æ¥æ± é—®é¢˜"""

    print("ğŸ” è¯Šæ–­è¿æ¥æ± é…ç½®")

    try:
        # åˆ›å»ºè¿æ¥æ± 
        pool = await asyncpg.create_pool(
            host="localhost",
            port=5432,
            database="postgres",
            user="postgres",
            password="postgres",
            min_size=2,
            max_size=10,
            command_timeout=10
        )

        print(f"âœ… è¿æ¥æ± åˆ›å»ºæˆåŠŸ")
        print(f"   æœ€å°è¿æ¥æ•°: {pool._minsize}")
        print(f"   æœ€å¤§è¿æ¥æ•°: {pool._maxsize}")

        # æµ‹è¯•å¹¶å‘è¿æ¥
        async def test_connection(conn_id):
            try:
                async with pool.acquire() as conn:
                    result = await conn.fetchval("SELECT $1", conn_id)
                    await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½
                    return f"è¿æ¥{conn_id}: æˆåŠŸ"
            except Exception as e:
                return f"è¿æ¥{conn_id}: å¤±è´¥ - {e}"

        # å¹¶å‘æµ‹è¯•
        tasks = [test_connection(i) for i in range(15)]
        results = await asyncio.gather(*tasks)

        successful = sum(1 for r in results if "æˆåŠŸ" in r)
        print(f"âœ… å¹¶å‘è¿æ¥æµ‹è¯•: {successful}/{len(tasks)} æˆåŠŸ")

        for result in results:
            if "å¤±è´¥" in result:
                print(f"   {result}")

        await pool.close()

    except Exception as e:
        print(f"âŒ è¿æ¥æ± æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(diagnose_postgresql_connection())
    asyncio.run(diagnose_connection_pool())
```

---

## ğŸ—‚ï¸ Redisç¼“å­˜é—®é¢˜

### Redisè¿æ¥å’Œæ€§èƒ½è¯Šæ–­

```python
#!/usr/bin/env python3
"""
Redisè¯Šæ–­è„šæœ¬
"""
import asyncio
import redis.asyncio as redis
import time
import json

async def diagnose_redis_connection():
    """è¯Šæ–­Redisè¿æ¥é—®é¢˜"""

    redis_configs = [
        {
            'name': 'æœ¬åœ°é»˜è®¤',
            'url': 'redis://localhost:6379'
        },
        {
            'name': 'æœ¬åœ°æ•°æ®åº“1',
            'url': 'redis://localhost:6379/1'
        },
        {
            'name': 'Docker Redis',
            'url': 'redis://127.0.0.1:6379'
        }
    ]

    for config in redis_configs:
        print(f"\nğŸ” æµ‹è¯• {config['name']}: {config['url']}")

        try:
            # åˆ›å»ºRediså®¢æˆ·ç«¯
            client = await redis.from_url(config['url'], decode_responses=True)

            # åŸºç¡€è¿æ¥æµ‹è¯•
            pong = await client.ping()
            print(f"âœ… è¿æ¥æˆåŠŸ: {pong}")

            # è·å–Redisä¿¡æ¯
            info = await client.info()
            print(f"âœ… Redisç‰ˆæœ¬: {info['redis_version']}")
            print(f"   ä½¿ç”¨å†…å­˜: {info['used_memory_human']}")
            print(f"   è¿æ¥æ•°: {info['connected_clients']}")
            print(f"   è¿è¡Œæ—¶é—´: {info['uptime_in_seconds']}ç§’")

            # æ€§èƒ½æµ‹è¯•
            test_key = "test_performance"
            test_value = {"data": "test" * 100}

            # å†™å…¥æ€§èƒ½
            start_time = time.time()
            await client.set(test_key, json.dumps(test_value), ex=60)
            write_latency = (time.time() - start_time) * 1000
            print(f"âœ… å†™å…¥å»¶è¿Ÿ: {write_latency:.2f}ms")

            # è¯»å–æ€§èƒ½
            start_time = time.time()
            cached_value = await client.get(test_key)
            read_latency = (time.time() - start_time) * 1000
            print(f"âœ… è¯»å–å»¶è¿Ÿ: {read_latency:.2f}ms")

            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            if cached_value == json.dumps(test_value):
                print(f"âœ… æ•°æ®ä¸€è‡´æ€§: æ­£ç¡®")
            else:
                print(f"âŒ æ•°æ®ä¸€è‡´æ€§: é”™è¯¯")

            # æ¸…ç†æµ‹è¯•æ•°æ®
            await client.delete(test_key)

            await client.close()

        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")

            if "Connection refused" in str(e):
                print("   å»ºè®®: æ£€æŸ¥Redisæ˜¯å¦è¿è¡Œ")
            elif "timeout" in str(e):
                print("   å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¢åŠ è¶…æ—¶æ—¶é—´")

async def diagnose_redis_memory_usage():
    """è¯Šæ–­Rediså†…å­˜ä½¿ç”¨"""

    try:
        client = await redis.from_url("redis://localhost:6379", decode_responses=True)

        print("\nğŸ” Rediså†…å­˜ä½¿ç”¨åˆ†æ")

        # è·å–å†…å­˜ä¿¡æ¯
        info = await client.info('memory')

        print(f"å·²ç”¨å†…å­˜: {info['used_memory_human']}")
        print(f"å†…å­˜å³°å€¼: {info['used_memory_peak_human']}")
        print(f"å†…å­˜ç¢ç‰‡ç‡: {info['mem_fragmentation_ratio']:.2f}")

        # åˆ†æé”®ç©ºé—´
        keyspace_info = await client.info('keyspace')
        for db, stats in keyspace_info.items():
            if db.startswith('db'):
                print(f"{db}: {stats}")

        # æ£€æŸ¥å¤§é”®
        print("\nğŸ” æ£€æŸ¥å¤§é”®:")
        cursor = 0
        large_keys = []

        while True:
            cursor, keys = await client.scan(cursor=cursor, count=100)

            for key in keys:
                try:
                    size = await client.memory_usage(key)
                    if size and size > 1024 * 1024:  # å¤§äº1MBçš„é”®
                        large_keys.append((key, size))
                except:
                    pass

            if cursor == 0:
                break

        if large_keys:
            print("å‘ç°å¤§é”®:")
            for key, size in sorted(large_keys, key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {key}: {size / 1024 / 1024:.2f}MB")
        else:
            print("æœªå‘ç°å¼‚å¸¸å¤§é”®")

        await client.close()

    except Exception as e:
        print(f"âŒ Rediså†…å­˜åˆ†æå¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(diagnose_redis_connection())
    asyncio.run(diagnose_redis_memory_usage())
```

---

## ğŸ’» ç³»ç»Ÿèµ„æºé—®é¢˜

### ç³»ç»Ÿèµ„æºç›‘æ§å’Œä¼˜åŒ–

```python
#!/usr/bin/env python3
"""
ç³»ç»Ÿèµ„æºç›‘æ§è„šæœ¬
"""
import psutil
import time
import platform
from datetime import datetime

def diagnose_system_resources():
    """è¯Šæ–­ç³»ç»Ÿèµ„æºçŠ¶å†µ"""

    print("ğŸ” ç³»ç»Ÿèµ„æºè¯Šæ–­")
    print("=" * 50)

    # ç³»ç»ŸåŸºæœ¬ä¿¡æ¯
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"æ¶æ„: {platform.machine()}")
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"å½“å‰æ—¶é—´: {datetime.now()}")

    print("\nğŸ’¾ å†…å­˜çŠ¶æ€:")
    memory = psutil.virtual_memory()
    print(f"æ€»å†…å­˜: {memory.total / 1024**3:.2f} GB")
    print(f"å¯ç”¨å†…å­˜: {memory.available / 1024**3:.2f} GB")
    print(f"å·²ç”¨å†…å­˜: {memory.used / 1024**3:.2f} GB ({memory.percent:.1f}%)")

    if memory.percent > 90:
        print("âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜!")
    elif memory.percent > 80:
        print("âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜")
    else:
        print("âœ… å†…å­˜ä½¿ç”¨ç‡æ­£å¸¸")

    print("\nğŸ”¥ CPUçŠ¶æ€:")
    cpu_count = psutil.cpu_count()
    cpu_count_logical = psutil.cpu_count(logical=True)
    print(f"ç‰©ç†æ ¸å¿ƒæ•°: {cpu_count}")
    print(f"é€»è¾‘æ ¸å¿ƒæ•°: {cpu_count_logical}")

    # CPUä½¿ç”¨ç‡ï¼ˆå¤šæ¬¡é‡‡æ ·å–å¹³å‡ï¼‰
    cpu_samples = []
    for i in range(5):
        cpu_percent = psutil.cpu_percent(interval=0.2)
        cpu_samples.append(cpu_percent)

    avg_cpu = sum(cpu_samples) / len(cpu_samples)
    print(f"å¹³å‡CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")

    if avg_cpu > 90:
        print("âš ï¸  CPUä½¿ç”¨ç‡è¿‡é«˜!")
    elif avg_cpu > 70:
        print("âš ï¸  CPUä½¿ç”¨ç‡è¾ƒé«˜")
    else:
        print("âœ… CPUä½¿ç”¨ç‡æ­£å¸¸")

    # æ¯æ ¸å¿ƒä½¿ç”¨ç‡
    per_cpu = psutil.cpu_percent(percpu=True, interval=1)
    print("å„æ ¸å¿ƒä½¿ç”¨ç‡:", [f"{cpu:.1f}%" for cpu in per_cpu])

    print("\nğŸ’¿ ç£ç›˜çŠ¶æ€:")
    disk_usage = psutil.disk_usage('/')
    print(f"æ€»ç©ºé—´: {disk_usage.total / 1024**3:.2f} GB")
    print(f"å·²ç”¨ç©ºé—´: {disk_usage.used / 1024**3:.2f} GB ({disk_usage.used/disk_usage.total*100:.1f}%)")
    print(f"å¯ç”¨ç©ºé—´: {disk_usage.free / 1024**3:.2f} GB")

    if disk_usage.used / disk_usage.total > 0.9:
        print("âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³!")
    elif disk_usage.used / disk_usage.total > 0.8:
        print("âš ï¸  ç£ç›˜ç©ºé—´è¾ƒå°‘")
    else:
        print("âœ… ç£ç›˜ç©ºé—´å……è¶³")

    # ç£ç›˜IO
    disk_io = psutil.disk_io_counters()
    if disk_io:
        print(f"ç£ç›˜è¯»å–: {disk_io.read_bytes / 1024**2:.2f} MB")
        print(f"ç£ç›˜å†™å…¥: {disk_io.write_bytes / 1024**2:.2f} MB")

    print("\nğŸŒ ç½‘ç»œçŠ¶æ€:")
    network_io = psutil.net_io_counters()
    if network_io:
        print(f"ç½‘ç»œå‘é€: {network_io.bytes_sent / 1024**2:.2f} MB")
        print(f"ç½‘ç»œæ¥æ”¶: {network_io.bytes_recv / 1024**2:.2f} MB")

    # ç½‘ç»œè¿æ¥
    connections = psutil.net_connections()
    tcp_connections = len([c for c in connections if c.type == 1])  # TCP
    print(f"TCPè¿æ¥æ•°: {tcp_connections}")

    print("\nğŸ”§ è¿›ç¨‹çŠ¶æ€:")
    # å½“å‰è¿›ç¨‹ä¿¡æ¯
    current_process = psutil.Process()
    print(f"å½“å‰è¿›ç¨‹PID: {current_process.pid}")
    print(f"è¿›ç¨‹å†…å­˜: {current_process.memory_info().rss / 1024**2:.2f} MB")
    print(f"è¿›ç¨‹CPU: {current_process.cpu_percent():.1f}%")

    # æŸ¥æ‰¾PostgreSQLå’ŒRedisè¿›ç¨‹
    postgres_processes = []
    redis_processes = []

    for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info']):
        try:
            if 'postgres' in proc.info['name'].lower():
                postgres_processes.append(proc.info)
            elif 'redis' in proc.info['name'].lower():
                redis_processes.append(proc.info)
        except:
            continue

    if postgres_processes:
        print(f"\nğŸ“Š PostgreSQLè¿›ç¨‹ ({len(postgres_processes)}ä¸ª):")
        for proc in postgres_processes[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            memory_mb = proc['memory_info'].rss / 1024**2 if proc['memory_info'] else 0
            print(f"  PID {proc['pid']}: CPU {proc['cpu_percent']:.1f}%, å†…å­˜ {memory_mb:.2f}MB")
    else:
        print("\nâŒ æœªæ‰¾åˆ°PostgreSQLè¿›ç¨‹")

    if redis_processes:
        print(f"\nğŸ—‚ï¸  Redisè¿›ç¨‹ ({len(redis_processes)}ä¸ª):")
        for proc in redis_processes:
            memory_mb = proc['memory_info'].rss / 1024**2 if proc['memory_info'] else 0
            print(f"  PID {proc['pid']}: CPU {proc['cpu_percent']:.1f}%, å†…å­˜ {memory_mb:.2f}MB")
    else:
        print("\nâŒ æœªæ‰¾åˆ°Redisè¿›ç¨‹")

def get_optimization_recommendations():
    """è·å–ç³»ç»Ÿä¼˜åŒ–å»ºè®®"""

    recommendations = []

    # å†…å­˜å»ºè®®
    memory = psutil.virtual_memory()
    if memory.percent > 90:
        recommendations.append("ğŸ”´ ç´§æ€¥: å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–åº”ç”¨ç¨‹åº")
    elif memory.percent > 80:
        recommendations.append("ğŸŸ¡ å»ºè®®: ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œè€ƒè™‘ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")

    # CPUå»ºè®®
    cpu_percent = psutil.cpu_percent(interval=1)
    if cpu_percent > 90:
        recommendations.append("ğŸ”´ ç´§æ€¥: CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ­»å¾ªç¯æˆ–ä¼˜åŒ–ç®—æ³•")
    elif cpu_percent > 70:
        recommendations.append("ğŸŸ¡ å»ºè®®: CPUä½¿ç”¨ç‡è¾ƒé«˜ï¼Œè€ƒè™‘ä¼˜åŒ–å¹¶å‘ç­–ç•¥")

    # ç£ç›˜å»ºè®®
    disk_usage = psutil.disk_usage('/')
    if disk_usage.used / disk_usage.total > 0.9:
        recommendations.append("ğŸ”´ ç´§æ€¥: ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶æˆ–æ‰©å®¹")
    elif disk_usage.used / disk_usage.total > 0.8:
        recommendations.append("ğŸŸ¡ å»ºè®®: ç£ç›˜ç©ºé—´è¾ƒå°‘ï¼Œå®šæœŸæ¸…ç†æ—¥å¿—æ–‡ä»¶")

    # è¿›ç¨‹å»ºè®®
    process_count = len(psutil.pids())
    if process_count > 1000:
        recommendations.append("ğŸŸ¡ å»ºè®®: è¿›ç¨‹æ•°è¾ƒå¤šï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åƒµå°¸è¿›ç¨‹")

    # ç½‘ç»œå»ºè®®
    connections = psutil.net_connections()
    tcp_count = len([c for c in connections if c.type == 1])
    if tcp_count > 1000:
        recommendations.append("ğŸŸ¡ å»ºè®®: TCPè¿æ¥æ•°è¾ƒå¤šï¼Œæ£€æŸ¥è¿æ¥æ± é…ç½®")

    if not recommendations:
        recommendations.append("âœ… ç³»ç»Ÿèµ„æºçŠ¶å†µè‰¯å¥½")

    return recommendations

if __name__ == "__main__":
    diagnose_system_resources()

    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print("=" * 50)
    for recommendation in get_optimization_recommendations():
        print(f"  {recommendation}")
```

---

## ğŸ“‹ æ—¥å¿—åˆ†æå’Œè¯Šæ–­

### è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æå·¥å…·

```python
#!/usr/bin/env python3
"""
æ—¥å¿—åˆ†æå’Œè¯Šæ–­å·¥å…·
"""
import re
import json
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict, Counter
import gzip

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""

    def __init__(self, log_directory="/h/novellus/test_results"):
        self.log_directory = Path(log_directory)
        self.error_patterns = {
            'connection_error': [
                r'Connection refused',
                r'Connection timeout',
                r'Connection reset by peer'
            ],
            'database_error': [
                r'relation .* does not exist',
                r'column .* does not exist',
                r'database .* does not exist'
            ],
            'authentication_error': [
                r'authentication failed',
                r'permission denied',
                r'invalid api key'
            ],
            'memory_error': [
                r'out of memory',
                r'memory allocation failed',
                r'cannot allocate memory'
            ],
            'timeout_error': [
                r'timeout',
                r'operation timed out',
                r'deadline exceeded'
            ]
        }

    def analyze_test_logs(self, hours_back=24):
        """åˆ†ææµ‹è¯•æ—¥å¿—"""

        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        analysis_results = {
            'log_files_analyzed': 0,
            'total_log_lines': 0,
            'error_summary': defaultdict(int),
            'error_details': defaultdict(list),
            'performance_issues': [],
            'test_failures': [],
            'recommendations': []
        }

        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        log_files = []
        log_files.extend(self.log_directory.glob("*.log"))
        log_files.extend(self.log_directory.glob("**/*.log"))
        log_files.extend(self.log_directory.glob("*.log.gz"))

        for log_file in log_files:
            if log_file.stat().st_mtime < cutoff_time.timestamp():
                continue

            try:
                analysis_results['log_files_analyzed'] += 1
                self._analyze_single_log_file(log_file, analysis_results)
            except Exception as e:
                print(f"åˆ†ææ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")

        # ç”Ÿæˆå»ºè®®
        analysis_results['recommendations'] = self._generate_recommendations(analysis_results)

        return analysis_results

    def _analyze_single_log_file(self, log_file, results):
        """åˆ†æå•ä¸ªæ—¥å¿—æ–‡ä»¶"""

        # è¯»å–æ—¥å¿—æ–‡ä»¶
        content = self._read_log_file(log_file)
        lines = content.split('\n')
        results['total_log_lines'] += len(lines)

        for line_num, line in enumerate(lines, 1):
            if not line.strip():
                continue

            # æ£€æŸ¥é”™è¯¯æ¨¡å¼
            for error_type, patterns in self.error_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        results['error_summary'][error_type] += 1
                        results['error_details'][error_type].append({
                            'file': str(log_file),
                            'line': line_num,
                            'content': line.strip(),
                            'timestamp': self._extract_timestamp(line)
                        })
                        break

            # æ£€æŸ¥æ€§èƒ½é—®é¢˜
            if self._is_performance_issue(line):
                results['performance_issues'].append({
                    'file': str(log_file),
                    'line': line_num,
                    'content': line.strip(),
                    'timestamp': self._extract_timestamp(line)
                })

            # æ£€æŸ¥æµ‹è¯•å¤±è´¥
            if self._is_test_failure(line):
                results['test_failures'].append({
                    'file': str(log_file),
                    'line': line_num,
                    'content': line.strip(),
                    'timestamp': self._extract_timestamp(line)
                })

    def _read_log_file(self, log_file):
        """è¯»å–æ—¥å¿—æ–‡ä»¶ï¼ˆæ”¯æŒå‹ç¼©æ–‡ä»¶ï¼‰"""

        if log_file.suffix == '.gz':
            with gzip.open(log_file, 'rt', encoding='utf-8') as f:
                return f.read()
        else:
            with open(log_file, 'r', encoding='utf-8') as f:
                return f.read()

    def _extract_timestamp(self, line):
        """æå–æ—¶é—´æˆ³"""

        # å°è¯•å¤šç§æ—¶é—´æˆ³æ ¼å¼
        timestamp_patterns = [
            r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}',
            r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',
            r'\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\]'
        ]

        for pattern in timestamp_patterns:
            match = re.search(pattern, line)
            if match:
                return match.group(0).strip('[]')

        return None

    def _is_performance_issue(self, line):
        """æ£€æŸ¥æ˜¯å¦æ˜¯æ€§èƒ½é—®é¢˜"""

        performance_indicators = [
            r'slow query',
            r'took \d+ms',
            r'timeout',
            r'high cpu',
            r'memory usage',
            r'latency'
        ]

        return any(re.search(pattern, line, re.IGNORECASE) for pattern in performance_indicators)

    def _is_test_failure(self, line):
        """æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•å¤±è´¥"""

        failure_indicators = [
            r'test.*failed',
            r'assertion.*failed',
            r'error.*in.*test',
            r'failed.*\d+.*test'
        ]

        return any(re.search(pattern, line, re.IGNORECASE) for pattern in failure_indicators)

    def _generate_recommendations(self, results):
        """ç”Ÿæˆè¯Šæ–­å»ºè®®"""

        recommendations = []

        # åŸºäºé”™è¯¯ç±»å‹çš„å»ºè®®
        if results['error_summary']['connection_error'] > 5:
            recommendations.append("ğŸ”´ é¢‘ç¹çš„è¿æ¥é”™è¯¯ï¼Œæ£€æŸ¥ç½‘ç»œé…ç½®å’ŒæœåŠ¡çŠ¶æ€")

        if results['error_summary']['database_error'] > 3:
            recommendations.append("ğŸ”´ æ•°æ®åº“é”™è¯¯ï¼Œæ£€æŸ¥æ•°æ®åº“æ¨¡å¼å’Œè¡¨ç»“æ„")

        if results['error_summary']['authentication_error'] > 0:
            recommendations.append("ğŸ”´ è®¤è¯é”™è¯¯ï¼Œæ£€æŸ¥APIå¯†é’¥å’Œæ•°æ®åº“å‡­è¯")

        if results['error_summary']['memory_error'] > 0:
            recommendations.append("ğŸ”´ å†…å­˜ä¸è¶³ï¼Œè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")

        if results['error_summary']['timeout_error'] > 10:
            recommendations.append("ğŸŸ¡ é¢‘ç¹è¶…æ—¶ï¼Œæ£€æŸ¥ç½‘ç»œå»¶è¿Ÿå’ŒæœåŠ¡æ€§èƒ½")

        # åŸºäºæ€§èƒ½é—®é¢˜çš„å»ºè®®
        if len(results['performance_issues']) > 20:
            recommendations.append("ğŸŸ¡ æ€§èƒ½é—®é¢˜è¾ƒå¤šï¼Œè€ƒè™‘ä¼˜åŒ–ç®—æ³•å’Œé…ç½®")

        # åŸºäºæµ‹è¯•å¤±è´¥çš„å»ºè®®
        if len(results['test_failures']) > 5:
            recommendations.append("ğŸŸ¡ æµ‹è¯•å¤±è´¥è¾ƒå¤šï¼Œæ£€æŸ¥æµ‹è¯•ç¯å¢ƒå’Œä¾èµ–")

        if not recommendations:
            recommendations.append("âœ… æœªå‘ç°ä¸¥é‡é—®é¢˜")

        return recommendations

    def generate_log_report(self, analysis_results):
        """ç”Ÿæˆæ—¥å¿—åˆ†ææŠ¥å‘Š"""

        report = f"""
# ğŸ“‹ æ—¥å¿—åˆ†ææŠ¥å‘Š

**åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ–‡ä»¶æ•°:** {analysis_results['log_files_analyzed']}
**æ€»æ—¥å¿—è¡Œæ•°:** {analysis_results['total_log_lines']}

## ğŸš¨ é”™è¯¯æ‘˜è¦

"""

        for error_type, count in analysis_results['error_summary'].items():
            if count > 0:
                report += f"- **{error_type}:** {count} æ¬¡\n"

        if analysis_results['performance_issues']:
            report += f"\n## âš¡ æ€§èƒ½é—®é¢˜\n\nå‘ç° {len(analysis_results['performance_issues'])} ä¸ªæ€§èƒ½ç›¸å…³é—®é¢˜\n"

        if analysis_results['test_failures']:
            report += f"\n## âŒ æµ‹è¯•å¤±è´¥\n\nå‘ç° {len(analysis_results['test_failures'])} ä¸ªæµ‹è¯•å¤±è´¥\n"

        report += "\n## ğŸ’¡ å»ºè®®\n\n"
        for recommendation in analysis_results['recommendations']:
            report += f"- {recommendation}\n"

        report += "\n## ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯\n\n"
        for error_type, details in analysis_results['error_details'].items():
            if details:
                report += f"### {error_type}\n\n"
                for detail in details[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    report += f"- **æ–‡ä»¶:** {detail['file']}\n"
                    report += f"  **è¡Œå·:** {detail['line']}\n"
                    report += f"  **å†…å®¹:** {detail['content'][:100]}...\n\n"

        return report

def main():
    """ä¸»å‡½æ•°"""

    analyzer = LogAnalyzer()

    print("ğŸ” å¼€å§‹åˆ†ææ—¥å¿—...")
    results = analyzer.analyze_test_logs(hours_back=24)

    print("\nğŸ“Š åˆ†æç»“æœ:")
    print(f"åˆ†æäº† {results['log_files_analyzed']} ä¸ªæ—¥å¿—æ–‡ä»¶")
    print(f"æ€»å…± {results['total_log_lines']} è¡Œæ—¥å¿—")

    if results['error_summary']:
        print("\nğŸš¨ é”™è¯¯ç»Ÿè®¡:")
        for error_type, count in results['error_summary'].items():
            print(f"  {error_type}: {count}")

    print(f"\nâš¡ æ€§èƒ½é—®é¢˜: {len(results['performance_issues'])}")
    print(f"âŒ æµ‹è¯•å¤±è´¥: {len(results['test_failures'])}")

    print("\nğŸ’¡ å»ºè®®:")
    for recommendation in results['recommendations']:
        print(f"  {recommendation}")

    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
    report = analyzer.generate_log_report(results)
    report_file = Path("/h/novellus/test_results/log_analysis_report.md")

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    main()
```

---

## ğŸ¯ å¿«é€Ÿè¯Šæ–­æ£€æŸ¥æ¸…å•

### æµ‹è¯•å¯åŠ¨å‰æ£€æŸ¥

```bash
#!/bin/bash
# å¿«é€Ÿè¯Šæ–­è„šæœ¬

echo "ğŸ¯ Novellusæµ‹è¯•ç³»ç»Ÿå¿«é€Ÿè¯Šæ–­"
echo "================================"

# 1. ç¯å¢ƒæ£€æŸ¥
echo "1ï¸âƒ£ æ£€æŸ¥Pythonç¯å¢ƒ..."
python3 --version || echo "âŒ Python3 æœªå®‰è£…"
pip3 show asyncpg > /dev/null 2>&1 && echo "âœ… asyncpg" || echo "âŒ asyncpg"
pip3 show redis > /dev/null 2>&1 && echo "âœ… redis" || echo "âŒ redis"

# 2. æ•°æ®åº“æ£€æŸ¥
echo "2ï¸âƒ£ æ£€æŸ¥æ•°æ®åº“è¿æ¥..."
pg_isready -h localhost -p 5432 && echo "âœ… PostgreSQL" || echo "âŒ PostgreSQL"
redis-cli ping > /dev/null 2>&1 && echo "âœ… Redis" || echo "âŒ Redis"

# 3. æ–‡ä»¶æ£€æŸ¥
echo "3ï¸âƒ£ æ£€æŸ¥å…³é”®æ–‡ä»¶..."
[ -f "/h/novellus/tests/test_pgvector_suite.py" ] && echo "âœ… pgvectoræµ‹è¯•" || echo "âŒ pgvectoræµ‹è¯•"
[ -f "/h/novellus/tests/test_ai_model_manager.py" ] && echo "âœ… AIæ¨¡å‹æµ‹è¯•" || echo "âŒ AIæ¨¡å‹æµ‹è¯•"
[ -f "/h/novellus/tests/automated_test_runner.py" ] && echo "âœ… è‡ªåŠ¨åŒ–æ‰§è¡Œå™¨" || echo "âŒ è‡ªåŠ¨åŒ–æ‰§è¡Œå™¨"

# 4. æƒé™æ£€æŸ¥
echo "4ï¸âƒ£ æ£€æŸ¥ç›®å½•æƒé™..."
[ -w "/h/novellus/test_results" ] && echo "âœ… ç»“æœç›®å½•å¯å†™" || echo "âŒ ç»“æœç›®å½•ä¸å¯å†™"

# 5. èµ„æºæ£€æŸ¥
echo "5ï¸âƒ£ æ£€æŸ¥ç³»ç»Ÿèµ„æº..."
MEMORY_USAGE=$(free | grep Mem | awk '{printf("%.1f"), $3/$2 * 100.0}')
echo "å†…å­˜ä½¿ç”¨ç‡: ${MEMORY_USAGE}%"
[ $(echo "$MEMORY_USAGE < 90" | bc -l) -eq 1 ] && echo "âœ… å†…å­˜å……è¶³" || echo "âš ï¸ å†…å­˜ä½¿ç”¨ç‡é«˜"

DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
echo "ç£ç›˜ä½¿ç”¨ç‡: ${DISK_USAGE}%"
[ $DISK_USAGE -lt 90 ] && echo "âœ… ç£ç›˜ç©ºé—´å……è¶³" || echo "âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³"

echo "================================"
echo "âœ… è¯Šæ–­å®Œæˆ"
```

---

## ğŸ“ è·å–æ”¯æŒ

å¦‚æœæŒ‰ç…§æœ¬æ•…éšœæ’é™¤æŒ‡å—ä»æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·ï¼š

1. **æ”¶é›†è¯Šæ–­ä¿¡æ¯:**
   ```bash
   # è¿è¡Œå®Œæ•´è¯Šæ–­
   python3 /h/novellus/tests/diagnose_environment.py > diagnosis.log 2>&1

   # æ”¶é›†æ—¥å¿—
   tar -czf novellus_logs.tar.gz /h/novellus/test_results/*.log
   ```

2. **è®°å½•é—®é¢˜è¯¦æƒ…:**
   - å…·ä½“é”™è¯¯ä¿¡æ¯
   - é‡ç°æ­¥éª¤
   - ç¯å¢ƒé…ç½®
   - ç›¸å…³æ—¥å¿—

3. **åˆ›å»ºé—®é¢˜æŠ¥å‘Š:**
   åŒ…å«ä¸Šè¿°è¯Šæ–­ä¿¡æ¯å’Œé—®é¢˜æè¿°

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æµ‹è¯•æ¶æ„æ–‡æ¡£](testing_architecture.md)
- [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](performance_optimization.md)
- [éƒ¨ç½²æŒ‡å—](deployment_guide.md)
- [APIæ–‡æ¡£](api_documentation.md)

---

*æœ€åæ›´æ–°: 2024å¹´9æœˆ20æ—¥*