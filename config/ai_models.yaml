# AI Models Configuration
# Define your AI models and their settings

models:
  # OpenAI Models
  - provider: openai
    model_name: gpt-4-turbo-preview
    display_name: GPT-4 Turbo
    description: Latest GPT-4 Turbo with 128k context window
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.01    # per 1K tokens
    output_token_cost: 0.03   # per 1K tokens
    supports_streaming: true
    supports_function_calling: true
    context_window: 128000
    requests_per_minute: 60
    priority: 90
    capabilities:
      - chat
      - completion
      - function_calling

  - provider: openai
    model_name: gpt-4o
    display_name: GPT-4o
    description: Multimodal GPT-4 optimized model
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.005
    output_token_cost: 0.015
    supports_streaming: true
    supports_function_calling: true
    context_window: 128000
    requests_per_minute: 60
    priority: 100
    capabilities:
      - chat
      - completion
      - image
      - function_calling

  - provider: openai
    model_name: gpt-4o-mini
    display_name: GPT-4o Mini
    description: Smaller, faster GPT-4o variant
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.00015
    output_token_cost: 0.0006
    supports_streaming: true
    supports_function_calling: true
    context_window: 128000
    requests_per_minute: 100
    priority: 80
    capabilities:
      - chat
      - completion
      - function_calling

  - provider: openai
    model_name: gpt-3.5-turbo
    display_name: GPT-3.5 Turbo
    description: Fast and cost-effective model
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.0005
    output_token_cost: 0.0015
    supports_streaming: true
    supports_function_calling: true
    context_window: 16385
    requests_per_minute: 90
    priority: 70
    capabilities:
      - chat
      - completion
      - function_calling

  # Anthropic Models
  - provider: anthropic
    model_name: claude-3-opus-20240229
    display_name: Claude 3 Opus
    description: Most capable Claude model
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.015
    output_token_cost: 0.075
    supports_streaming: true
    supports_function_calling: false
    context_window: 200000
    requests_per_minute: 50
    priority: 95
    capabilities:
      - chat
      - completion

  - provider: anthropic
    model_name: claude-3-sonnet-20240229
    display_name: Claude 3 Sonnet
    description: Balanced Claude model
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.003
    output_token_cost: 0.015
    supports_streaming: true
    supports_function_calling: false
    context_window: 200000
    requests_per_minute: 60
    priority: 85
    capabilities:
      - chat
      - completion

  - provider: anthropic
    model_name: claude-3-haiku-20240307
    display_name: Claude 3 Haiku
    description: Fast Claude model
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.00025
    output_token_cost: 0.00125
    supports_streaming: true
    supports_function_calling: false
    context_window: 200000
    requests_per_minute: 80
    priority: 75
    capabilities:
      - chat
      - completion

  # Local/Ollama Models
  - provider: ollama
    model_name: llama3.1:70b
    display_name: Llama 3.1 70B
    description: Large Llama 3.1 model
    api_endpoint: http://localhost:11434
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.0
    output_token_cost: 0.0
    supports_streaming: true
    supports_function_calling: false
    context_window: 8192
    requests_per_minute: 30
    priority: 60
    capabilities:
      - chat
      - completion

  - provider: ollama
    model_name: mixtral:8x7b
    display_name: Mixtral 8x7B
    description: Mixture of experts model
    api_endpoint: http://localhost:11434
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.0
    output_token_cost: 0.0
    supports_streaming: true
    supports_function_calling: false
    context_window: 32768
    requests_per_minute: 30
    priority: 65
    capabilities:
      - chat
      - completion

  - provider: ollama
    model_name: qwen2.5:72b
    display_name: Qwen 2.5 72B
    description: Alibaba's Qwen model
    api_endpoint: http://localhost:11434
    max_tokens: 4096
    temperature: 0.7
    input_token_cost: 0.0
    output_token_cost: 0.0
    supports_streaming: true
    supports_function_calling: false
    context_window: 32768
    requests_per_minute: 30
    priority: 55
    capabilities:
      - chat
      - completion

# Load Balancing Rules
load_balancing:
  default_strategy: weighted  # weighted, round_robin, least_latency, cost_optimized

  rules:
    - name: high_priority_routing
      description: Route high-priority requests to best models
      strategy: weighted
      model_pool:
        - gpt-4o
        - claude-3-opus-20240229
        - gpt-4-turbo-preview
      conditions:
        priority: high
        max_latency_ms: 5000

    - name: cost_optimized_routing
      description: Route to most cost-effective models
      strategy: cost_optimized
      model_pool:
        - gpt-3.5-turbo
        - claude-3-haiku-20240307
        - gpt-4o-mini
      conditions:
        priority: low
        budget_conscious: true

    - name: local_first_routing
      description: Prefer local models when available
      strategy: priority
      model_pool:
        - llama3.1:70b
        - mixtral:8x7b
        - qwen2.5:72b
      conditions:
        prefer_local: true

    - name: function_calling_routing
      description: Route to models that support function calling
      strategy: weighted
      model_pool:
        - gpt-4o
        - gpt-4-turbo-preview
        - gpt-3.5-turbo
      conditions:
        requires_functions: true

# Cache Settings
cache:
  enabled: true
  ttl_seconds: 3600
  max_memory_items: 1000
  semantic_cache:
    enabled: true
    embedding_model: text-embedding-3-small
    similarity_threshold: 0.85
    max_semantic_items: 5000

# Monitoring Settings
monitoring:
  metrics_window_seconds: 3600
  anomaly_detection:
    enabled: true
    latency_threshold_multiplier: 2.0
    error_rate_threshold: 5.0
  health_check_interval: 60
  prometheus:
    enabled: true
    port: 9090