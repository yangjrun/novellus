#!/usr/bin/env python3
"""
Performance Benchmark and Monitoring Suite
æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œç›‘æ§ç³»ç»Ÿï¼Œç”¨äºæµ‹è¯•pgvectorã€AIæ¨¡å‹ç®¡ç†ç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡
"""

import asyncio
import json
import logging
import os
import random
import statistics
import time
import uuid
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
import traceback

import asyncpg
import numpy as np
import psutil
import matplotlib.pyplot as plt
import seaborn as sns
from contextlib import asynccontextmanager

# å¯¼å…¥è¢«æµ‹è¯•çš„æ¨¡å—
import sys
sys.path.append('/h/novellus/src')

from ai.model_manager import AIModelManager, ModelConfig, ModelProvider, ModelStatus
from ai.cache_manager import CacheManager
from ai.metrics_collector import MetricsCollector
from config import config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    db_host: str = "localhost"
    db_port: int = 5432
    db_name: str = "postgres"
    db_user: str = "postgres"
    db_password: str = "postgres"
    redis_url: str = "redis://localhost:6379"

    # æµ‹è¯•è§„æ¨¡é…ç½®
    vector_dimensions: int = 1536
    small_dataset_size: int = 1000
    medium_dataset_size: int = 10000
    large_dataset_size: int = 100000

    # æ€§èƒ½æµ‹è¯•é…ç½®
    concurrent_users: List[int] = field(default_factory=lambda: [1, 5, 10, 20, 50])
    query_batches: List[int] = field(default_factory=lambda: [10, 50, 100, 500])
    cache_sizes: List[int] = field(default_factory=lambda: [100, 500, 1000, 5000])

    # åŸºå‡†é˜ˆå€¼
    vector_search_threshold_ms: int = 100
    ai_completion_threshold_ms: int = 2000
    cache_hit_threshold_ms: int = 50
    throughput_threshold_qps: float = 10.0

    # ç›‘æ§é…ç½®
    monitoring_duration_seconds: int = 300
    metrics_collection_interval_seconds: int = 5

    # è¾“å‡ºé…ç½®
    generate_plots: bool = True
    save_raw_data: bool = True

@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡"""
    metric_name: str
    timestamp: datetime
    value: float
    unit: str
    component: str
    test_scenario: str
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    success: bool
    metrics: List[PerformanceMetric]
    summary_stats: Dict[str, Any]
    error_message: str = ""

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""

    def __init__(self, collection_interval: int = 5):
        self.collection_interval = collection_interval
        self.monitoring = False
        self.metrics_history: List[Dict[str, Any]] = []

    async def start_monitoring(self):
        """å¼€å§‹ç³»ç»Ÿç›‘æ§"""
        self.monitoring = True
        self.metrics_history = []

        while self.monitoring:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                process = psutil.Process()

                # CPUå’Œå†…å­˜
                cpu_percent = psutil.cpu_percent(interval=1)
                memory_info = psutil.virtual_memory()
                process_memory = process.memory_info()

                # ç£ç›˜IO
                disk_io = psutil.disk_io_counters()

                # ç½‘ç»œIO
                network_io = psutil.net_io_counters()

                metrics = {
                    'timestamp': datetime.now().isoformat(),
                    'cpu_percent': cpu_percent,
                    'memory_percent': memory_info.percent,
                    'memory_available_gb': memory_info.available / (1024**3),
                    'process_memory_mb': process_memory.rss / (1024**2),
                    'disk_read_mb': disk_io.read_bytes / (1024**2) if disk_io else 0,
                    'disk_write_mb': disk_io.write_bytes / (1024**2) if disk_io else 0,
                    'network_sent_mb': network_io.bytes_sent / (1024**2) if network_io else 0,
                    'network_recv_mb': network_io.bytes_recv / (1024**2) if network_io else 0
                }

                self.metrics_history.append(metrics)

                await asyncio.sleep(self.collection_interval)

            except Exception as e:
                logger.error(f"System monitoring error: {e}")
                await asyncio.sleep(self.collection_interval)

    def stop_monitoring(self):
        """åœæ­¢ç³»ç»Ÿç›‘æ§"""
        self.monitoring = False

    def get_metrics_summary(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æŒ‡æ ‡æ‘˜è¦"""
        if not self.metrics_history:
            return {}

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        cpu_values = [m['cpu_percent'] for m in self.metrics_history]
        memory_values = [m['memory_percent'] for m in self.metrics_history]
        process_memory_values = [m['process_memory_mb'] for m in self.metrics_history]

        return {
            'monitoring_duration_seconds': len(self.metrics_history) * self.collection_interval,
            'cpu_stats': {
                'min': min(cpu_values),
                'max': max(cpu_values),
                'avg': statistics.mean(cpu_values),
                'median': statistics.median(cpu_values)
            },
            'memory_stats': {
                'min': min(memory_values),
                'max': max(memory_values),
                'avg': statistics.mean(memory_values),
                'median': statistics.median(memory_values)
            },
            'process_memory_stats': {
                'min': min(process_memory_values),
                'max': max(process_memory_values),
                'avg': statistics.mean(process_memory_values),
                'median': statistics.median(process_memory_values)
            },
            'raw_metrics': self.metrics_history
        }

class PerformanceBenchmarkSuite:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""

    def __init__(self, config: BenchmarkConfig = None):
        self.config = config or BenchmarkConfig()
        self.db_pool = None
        self.ai_manager = None
        self.cache_manager = None
        self.metrics_collector = None
        self.system_monitor = SystemMonitor(self.config.metrics_collection_interval_seconds)

        # ç»“æœå­˜å‚¨
        self.benchmark_results: List[BenchmarkResult] = []
        self.performance_metrics: List[PerformanceMetric] = []

        # æµ‹è¯•æ•°æ®
        self.test_data_ids: List[str] = []

    async def initialize(self):
        """åˆå§‹åŒ–åŸºå‡†æµ‹è¯•ç¯å¢ƒ"""
        logger.info("Initializing performance benchmark suite...")

        # åˆ›å»ºæ•°æ®åº“è¿æ¥æ± 
        self.db_pool = await asyncpg.create_pool(
            host=self.config.db_host,
            port=self.config.db_port,
            database=self.config.db_name,
            user=self.config.db_user,
            password=self.config.db_password,
            min_size=10,
            max_size=50
        )

        # åˆå§‹åŒ–AIæ¨¡å‹ç®¡ç†å™¨
        self.ai_manager = AIModelManager(
            db_url=f"postgresql+asyncpg://{self.config.db_user}:{self.config.db_password}@{self.config.db_host}:{self.config.db_port}/{self.config.db_name}",
            redis_url=self.config.redis_url
        )

        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = CacheManager(
            redis_url=self.config.redis_url,
            db_pool=self.db_pool
        )

        # åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨
        self.metrics_collector = MetricsCollector(
            db_pool=self.db_pool,
            collection_interval=self.config.metrics_collection_interval_seconds
        )

        # å¯åŠ¨ç»„ä»¶
        await self.ai_manager.initialize()
        await self.cache_manager.initialize()
        await self.metrics_collector.initialize()

        # éªŒè¯ç¯å¢ƒ
        await self._verify_benchmark_environment()

        logger.info("Performance benchmark suite initialized successfully")

    async def _verify_benchmark_environment(self):
        """éªŒè¯åŸºå‡†æµ‹è¯•ç¯å¢ƒ"""
        # éªŒè¯pgvectoræ‰©å±•
        async with self.db_pool.acquire() as conn:
            vector_ext = await conn.fetchval("SELECT COUNT(*) FROM pg_extension WHERE extname = 'vector'")
            if vector_ext == 0:
                raise Exception("pgvector extension not available for benchmarking")

            # åˆ›å»ºåŸºå‡†æµ‹è¯•è¡¨
            await conn.execute(f"""
                CREATE TABLE IF NOT EXISTS benchmark_vectors (
                    id BIGSERIAL PRIMARY KEY,
                    dataset_size INTEGER,
                    batch_id INTEGER,
                    content_text TEXT,
                    embedding vector({self.config.vector_dimensions}),
                    metadata JSONB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡è¡¨
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS performance_benchmarks (
                    id SERIAL PRIMARY KEY,
                    test_name VARCHAR(255),
                    component VARCHAR(100),
                    test_scenario VARCHAR(255),
                    metric_name VARCHAR(100),
                    metric_value NUMERIC,
                    metric_unit VARCHAR(50),
                    metadata JSONB,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

    def _generate_test_vectors(self, count: int, batch_id: int = 0) -> List[Tuple[str, List[float], Dict[str, Any]]]:
        """ç”Ÿæˆæµ‹è¯•å‘é‡æ•°æ®"""
        vectors = []

        for i in range(count):
            # ç”Ÿæˆéšæœºå‘é‡
            embedding = [random.random() for _ in range(self.config.vector_dimensions)]

            # ç”Ÿæˆæµ‹è¯•å†…å®¹
            content = f"Benchmark test content {batch_id}_{i}: " + " ".join([
                f"word{j}" for j in range(random.randint(10, 50))
            ])

            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = {
                'batch_id': batch_id,
                'content_length': len(content),
                'vector_norm': sum(x*x for x in embedding) ** 0.5,
                'generated_at': datetime.now().isoformat()
            }

            vectors.append((content, embedding, metadata))

        return vectors

    async def _insert_benchmark_vectors(self, dataset_size: int, batch_size: int = 1000) -> int:
        """æ’å…¥åŸºå‡†æµ‹è¯•å‘é‡æ•°æ®"""
        logger.info(f"Inserting {dataset_size} benchmark vectors...")

        total_inserted = 0
        batch_id = 0

        for i in range(0, dataset_size, batch_size):
            current_batch_size = min(batch_size, dataset_size - i)
            vectors = self._generate_test_vectors(current_batch_size, batch_id)

            async with self.db_pool.acquire() as conn:
                insert_data = [
                    (dataset_size, batch_id, content, embedding, json.dumps(metadata))
                    for content, embedding, metadata in vectors
                ]

                await conn.executemany("""
                    INSERT INTO benchmark_vectors (dataset_size, batch_id, content_text, embedding, metadata)
                    VALUES ($1, $2, $3, $4, $5)
                """, insert_data)

            total_inserted += current_batch_size
            batch_id += 1

            if batch_id % 10 == 0:
                logger.info(f"Inserted {total_inserted}/{dataset_size} vectors...")

        logger.info(f"Completed inserting {total_inserted} benchmark vectors")
        return total_inserted

    async def benchmark_vector_search_performance(self) -> BenchmarkResult:
        """åŸºå‡†æµ‹è¯•å‘é‡æœç´¢æ€§èƒ½"""
        test_name = "vector_search_performance"
        start_time = datetime.now()
        metrics = []

        try:
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_dataset_sizes = [
                self.config.small_dataset_size,
                self.config.medium_dataset_size,
                self.config.large_dataset_size
            ]

            for dataset_size in test_dataset_sizes:
                logger.info(f"Benchmarking vector search with {dataset_size} vectors...")

                # æ’å…¥æµ‹è¯•æ•°æ®
                await self._insert_benchmark_vectors(dataset_size)

                # åˆ›å»ºç´¢å¼•å¹¶æµ‹è¯•ä¸åŒç´¢å¼•ç±»å‹
                index_types = [
                    ('no_index', None),
                    ('hnsw', 'CREATE INDEX CONCURRENTLY idx_benchmark_hnsw ON benchmark_vectors USING hnsw (embedding vector_l2_ops)'),
                    ('ivfflat', 'CREATE INDEX CONCURRENTLY idx_benchmark_ivfflat ON benchmark_vectors USING ivfflat (embedding vector_l2_ops) WITH (lists = 100)')
                ]

                for index_name, index_sql in index_types:
                    # åˆ›å»ºç´¢å¼•
                    if index_sql:
                        async with self.db_pool.acquire() as conn:
                            await conn.execute("DROP INDEX IF EXISTS idx_benchmark_hnsw")
                            await conn.execute("DROP INDEX IF EXISTS idx_benchmark_ivfflat")
                            await conn.execute(index_sql)

                    # æµ‹è¯•ä¸åŒçš„æŸ¥è¯¢æ‰¹æ¬¡å¤§å°
                    for batch_size in self.config.query_batches:
                        # ç”ŸæˆæŸ¥è¯¢å‘é‡
                        query_vectors = [
                            [random.random() for _ in range(self.config.vector_dimensions)]
                            for _ in range(batch_size)
                        ]

                        # æ‰§è¡ŒæŸ¥è¯¢æ€§èƒ½æµ‹è¯•
                        latencies = []
                        throughput_start = time.time()

                        for query_vector in query_vectors:
                            query_start = time.time()

                            async with self.db_pool.acquire() as conn:
                                results = await conn.fetch("""
                                    SELECT id, content_text, embedding <-> $1 as distance
                                    FROM benchmark_vectors
                                    WHERE dataset_size = $2
                                    ORDER BY embedding <-> $1
                                    LIMIT 10
                                """, query_vector, dataset_size)

                            query_latency = (time.time() - query_start) * 1000
                            latencies.append(query_latency)

                        throughput_duration = time.time() - throughput_start
                        throughput = batch_size / throughput_duration

                        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                        latencies.sort()

                        metrics.extend([
                            PerformanceMetric(
                                metric_name="vector_search_latency_p50",
                                timestamp=datetime.now(),
                                value=latencies[len(latencies) // 2],
                                unit="ms",
                                component="pgvector",
                                test_scenario=f"{dataset_size}_{index_name}_{batch_size}",
                                metadata={
                                    'dataset_size': dataset_size,
                                    'index_type': index_name,
                                    'batch_size': batch_size
                                }
                            ),
                            PerformanceMetric(
                                metric_name="vector_search_latency_p95",
                                timestamp=datetime.now(),
                                value=latencies[int(len(latencies) * 0.95)],
                                unit="ms",
                                component="pgvector",
                                test_scenario=f"{dataset_size}_{index_name}_{batch_size}"
                            ),
                            PerformanceMetric(
                                metric_name="vector_search_throughput",
                                timestamp=datetime.now(),
                                value=throughput,
                                unit="qps",
                                component="pgvector",
                                test_scenario=f"{dataset_size}_{index_name}_{batch_size}"
                            )
                        ])

                        logger.info(f"Dataset: {dataset_size}, Index: {index_name}, Batch: {batch_size}, "
                                   f"P50: {latencies[len(latencies) // 2]:.2f}ms, "
                                   f"Throughput: {throughput:.2f} qps")

                # æ¸…ç†æµ‹è¯•æ•°æ®
                async with self.db_pool.acquire() as conn:
                    await conn.execute("DELETE FROM benchmark_vectors WHERE dataset_size = $1", dataset_size)
                    await conn.execute("DROP INDEX IF EXISTS idx_benchmark_hnsw")
                    await conn.execute("DROP INDEX IF EXISTS idx_benchmark_ivfflat")

            # è®¡ç®—æ‘˜è¦ç»Ÿè®¡
            search_latencies = [m.value for m in metrics if 'latency' in m.metric_name]
            search_throughputs = [m.value for m in metrics if 'throughput' in m.metric_name]

            summary_stats = {
                'total_tests': len(metrics),
                'avg_latency_ms': statistics.mean(search_latencies) if search_latencies else 0,
                'max_latency_ms': max(search_latencies) if search_latencies else 0,
                'avg_throughput_qps': statistics.mean(search_throughputs) if search_throughputs else 0,
                'max_throughput_qps': max(search_throughputs) if search_throughputs else 0,
                'performance_acceptable': all(l < self.config.vector_search_threshold_ms for l in search_latencies) if search_latencies else False
            }

            return BenchmarkResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                success=True,
                metrics=metrics,
                summary_stats=summary_stats
            )

        except Exception as e:
            logger.error(f"Vector search benchmark failed: {e}")
            return BenchmarkResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                success=False,
                metrics=metrics,
                summary_stats={},
                error_message=str(e)
            )

    async def benchmark_ai_model_performance(self) -> BenchmarkResult:
        """åŸºå‡†æµ‹è¯•AIæ¨¡å‹æ€§èƒ½"""
        test_name = "ai_model_performance"
        start_time = datetime.now()
        metrics = []

        try:
            # æµ‹è¯•æç¤ºè¯é›†åˆ
            test_prompts = [
                ("short", "Hello, how are you?"),
                ("medium", "Explain the concept of artificial intelligence and its applications in modern technology."),
                ("long", "Write a comprehensive analysis of the impact of machine learning on various industries, including healthcare, finance, education, and transportation. Discuss both benefits and challenges." * 3),
                ("code", "Write a Python function to calculate the factorial of a number using recursion."),
                ("creative", "Write a short story about a robot learning to understand human emotions.")
            ]

            # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
            for concurrent_users in self.config.concurrent_users:
                logger.info(f"Testing AI model performance with {concurrent_users} concurrent users...")

                for prompt_type, prompt in test_prompts:
                    async def single_completion(user_id: int):
                        completion_start = time.time()

                        try:
                            # æ¨¡æ‹ŸAIå®Œæˆè¯·æ±‚
                            result = {
                                'content': f"Mock AI response for user {user_id}: {prompt[:50]}...",
                                'model': 'mock-gpt-4',
                                'usage': {
                                    'prompt_tokens': len(prompt.split()),
                                    'completion_tokens': 100,
                                    'total_tokens': len(prompt.split()) + 100
                                }
                            }

                            completion_time = (time.time() - completion_start) * 1000

                            return {
                                'user_id': user_id,
                                'success': True,
                                'latency_ms': completion_time,
                                'tokens': result['usage']['total_tokens'],
                                'cost': result['usage']['total_tokens'] * 0.00001  # æ¨¡æ‹Ÿæˆæœ¬
                            }

                        except Exception as e:
                            completion_time = (time.time() - completion_start) * 1000
                            return {
                                'user_id': user_id,
                                'success': False,
                                'latency_ms': completion_time,
                                'error': str(e)
                            }

                    # æ‰§è¡Œå¹¶å‘æµ‹è¯•
                    batch_start = time.time()
                    tasks = [single_completion(i) for i in range(concurrent_users)]
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    batch_duration = time.time() - batch_start

                    # åˆ†æç»“æœ
                    successful_results = [r for r in results if isinstance(r, dict) and r.get('success')]
                    failed_results = [r for r in results if not isinstance(r, dict) or not r.get('success')]

                    if successful_results:
                        latencies = [r['latency_ms'] for r in successful_results]
                        latencies.sort()

                        total_tokens = sum(r.get('tokens', 0) for r in successful_results)
                        total_cost = sum(r.get('cost', 0) for r in successful_results)
                        throughput = len(successful_results) / batch_duration

                        metrics.extend([
                            PerformanceMetric(
                                metric_name="ai_completion_latency_p50",
                                timestamp=datetime.now(),
                                value=latencies[len(latencies) // 2],
                                unit="ms",
                                component="ai_model",
                                test_scenario=f"{prompt_type}_{concurrent_users}",
                                metadata={
                                    'prompt_type': prompt_type,
                                    'concurrent_users': concurrent_users,
                                    'prompt_length': len(prompt)
                                }
                            ),
                            PerformanceMetric(
                                metric_name="ai_completion_latency_p95",
                                timestamp=datetime.now(),
                                value=latencies[int(len(latencies) * 0.95)],
                                unit="ms",
                                component="ai_model",
                                test_scenario=f"{prompt_type}_{concurrent_users}"
                            ),
                            PerformanceMetric(
                                metric_name="ai_completion_throughput",
                                timestamp=datetime.now(),
                                value=throughput,
                                unit="rps",
                                component="ai_model",
                                test_scenario=f"{prompt_type}_{concurrent_users}"
                            ),
                            PerformanceMetric(
                                metric_name="ai_completion_cost",
                                timestamp=datetime.now(),
                                value=total_cost,
                                unit="usd",
                                component="ai_model",
                                test_scenario=f"{prompt_type}_{concurrent_users}"
                            )
                        ])

                        logger.info(f"Prompt: {prompt_type}, Users: {concurrent_users}, "
                                   f"P50: {latencies[len(latencies) // 2]:.2f}ms, "
                                   f"Success Rate: {len(successful_results)}/{concurrent_users}")

            # è®¡ç®—æ‘˜è¦ç»Ÿè®¡
            completion_latencies = [m.value for m in metrics if 'latency' in m.metric_name]
            completion_throughputs = [m.value for m in metrics if 'throughput' in m.metric_name]
            total_costs = [m.value for m in metrics if 'cost' in m.metric_name]

            summary_stats = {
                'total_tests': len(metrics),
                'avg_latency_ms': statistics.mean(completion_latencies) if completion_latencies else 0,
                'max_latency_ms': max(completion_latencies) if completion_latencies else 0,
                'avg_throughput_rps': statistics.mean(completion_throughputs) if completion_throughputs else 0,
                'total_cost_usd': sum(total_costs),
                'performance_acceptable': all(l < self.config.ai_completion_threshold_ms for l in completion_latencies) if completion_latencies else False
            }

            return BenchmarkResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                success=True,
                metrics=metrics,
                summary_stats=summary_stats
            )

        except Exception as e:
            logger.error(f"AI model benchmark failed: {e}")
            return BenchmarkResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                success=False,
                metrics=metrics,
                summary_stats={},
                error_message=str(e)
            )

    async def benchmark_cache_performance(self) -> BenchmarkResult:
        """åŸºå‡†æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        test_name = "cache_performance"
        start_time = datetime.now()
        metrics = []

        try:
            # æµ‹è¯•ä¸åŒç¼“å­˜å¤§å°
            for cache_size in self.config.cache_sizes:
                logger.info(f"Testing cache performance with {cache_size} items...")

                # é¢„å¡«å……ç¼“å­˜
                cache_data = {}
                for i in range(cache_size):
                    key = f"benchmark_key_{i}"
                    value = {
                        'data': f"Benchmark cache value {i}",
                        'metadata': {'size': len(f"Benchmark cache value {i}"), 'index': i},
                        'created_at': datetime.now().isoformat()
                    }
                    cache_data[key] = value

                # å†™å…¥æ€§èƒ½æµ‹è¯•
                write_start = time.time()
                write_latencies = []

                for key, value in cache_data.items():
                    write_op_start = time.time()

                    # æ¨¡æ‹Ÿç¼“å­˜å†™å…¥
                    await asyncio.sleep(0.001)  # æ¨¡æ‹Ÿå†™å…¥å»¶è¿Ÿ

                    write_latency = (time.time() - write_op_start) * 1000
                    write_latencies.append(write_latency)

                total_write_time = time.time() - write_start
                write_throughput = cache_size / total_write_time

                # è¯»å–æ€§èƒ½æµ‹è¯•
                read_start = time.time()
                read_latencies = []
                cache_hits = 0

                # æµ‹è¯•ç¼“å­˜å‘½ä¸­
                for key in list(cache_data.keys())[:cache_size // 2]:
                    read_op_start = time.time()

                    # æ¨¡æ‹Ÿç¼“å­˜è¯»å–
                    if key in cache_data:
                        cache_hits += 1
                        await asyncio.sleep(0.0005)  # æ¨¡æ‹Ÿç¼“å­˜å‘½ä¸­å»¶è¿Ÿ
                    else:
                        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿç¼“å­˜æœªå‘½ä¸­å»¶è¿Ÿ

                    read_latency = (time.time() - read_op_start) * 1000
                    read_latencies.append(read_latency)

                # æµ‹è¯•ç¼“å­˜æœªå‘½ä¸­
                for i in range(cache_size // 2):
                    key = f"nonexistent_key_{i}"
                    read_op_start = time.time()

                    # æ¨¡æ‹Ÿç¼“å­˜æœªå‘½ä¸­
                    await asyncio.sleep(0.01)

                    read_latency = (time.time() - read_op_start) * 1000
                    read_latencies.append(read_latency)

                total_read_time = time.time() - read_start
                read_throughput = len(read_latencies) / total_read_time
                cache_hit_rate = cache_hits / (cache_size // 2)

                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                write_latencies.sort()
                read_latencies.sort()

                metrics.extend([
                    PerformanceMetric(
                        metric_name="cache_write_latency_p50",
                        timestamp=datetime.now(),
                        value=write_latencies[len(write_latencies) // 2],
                        unit="ms",
                        component="cache",
                        test_scenario=f"size_{cache_size}",
                        metadata={'cache_size': cache_size}
                    ),
                    PerformanceMetric(
                        metric_name="cache_read_latency_p50",
                        timestamp=datetime.now(),
                        value=read_latencies[len(read_latencies) // 2],
                        unit="ms",
                        component="cache",
                        test_scenario=f"size_{cache_size}"
                    ),
                    PerformanceMetric(
                        metric_name="cache_write_throughput",
                        timestamp=datetime.now(),
                        value=write_throughput,
                        unit="ops/s",
                        component="cache",
                        test_scenario=f"size_{cache_size}"
                    ),
                    PerformanceMetric(
                        metric_name="cache_read_throughput",
                        timestamp=datetime.now(),
                        value=read_throughput,
                        unit="ops/s",
                        component="cache",
                        test_scenario=f"size_{cache_size}"
                    ),
                    PerformanceMetric(
                        metric_name="cache_hit_rate",
                        timestamp=datetime.now(),
                        value=cache_hit_rate * 100,
                        unit="percent",
                        component="cache",
                        test_scenario=f"size_{cache_size}"
                    )
                ])

                logger.info(f"Cache size: {cache_size}, "
                           f"Write P50: {write_latencies[len(write_latencies) // 2]:.2f}ms, "
                           f"Read P50: {read_latencies[len(read_latencies) // 2]:.2f}ms, "
                           f"Hit Rate: {cache_hit_rate:.2%}")

            # è®¡ç®—æ‘˜è¦ç»Ÿè®¡
            write_latencies = [m.value for m in metrics if 'write_latency' in m.metric_name]
            read_latencies = [m.value for m in metrics if 'read_latency' in m.metric_name]
            hit_rates = [m.value for m in metrics if 'hit_rate' in m.metric_name]

            summary_stats = {
                'total_tests': len(metrics),
                'avg_write_latency_ms': statistics.mean(write_latencies) if write_latencies else 0,
                'avg_read_latency_ms': statistics.mean(read_latencies) if read_latencies else 0,
                'avg_hit_rate_percent': statistics.mean(hit_rates) if hit_rates else 0,
                'performance_acceptable': all(l < self.config.cache_hit_threshold_ms for l in read_latencies) if read_latencies else False
            }

            return BenchmarkResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                success=True,
                metrics=metrics,
                summary_stats=summary_stats
            )

        except Exception as e:
            logger.error(f"Cache benchmark failed: {e}")
            return BenchmarkResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                success=False,
                metrics=metrics,
                summary_stats={},
                error_message=str(e)
            )

    async def benchmark_system_load_limits(self) -> BenchmarkResult:
        """åŸºå‡†æµ‹è¯•ç³»ç»Ÿè´Ÿè½½æé™"""
        test_name = "system_load_limits"
        start_time = datetime.now()
        metrics = []

        try:
            # å¯åŠ¨ç³»ç»Ÿç›‘æ§
            monitor_task = asyncio.create_task(self.system_monitor.start_monitoring())

            # é€æ­¥å¢åŠ è´Ÿè½½ç›´åˆ°ç³»ç»Ÿè¾¾åˆ°æé™
            load_levels = [10, 25, 50, 100, 200, 500, 1000]

            for load_level in load_levels:
                logger.info(f"Testing system load limit at {load_level} concurrent operations...")

                async def load_operation(op_id: int):
                    operation_start = time.time()

                    try:
                        # æ¨¡æ‹Ÿå¤åˆæ“ä½œï¼šå‘é‡æœç´¢ + AIå®Œæˆ + ç¼“å­˜æ“ä½œ

                        # 1. å‘é‡æœç´¢
                        query_vector = [random.random() for _ in range(self.config.vector_dimensions)]
                        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå‘é‡æœç´¢å»¶è¿Ÿ

                        # 2. AIå®Œæˆ
                        await asyncio.sleep(0.1)  # æ¨¡æ‹ŸAIå®Œæˆå»¶è¿Ÿ

                        # 3. ç¼“å­˜æ“ä½œ
                        await asyncio.sleep(0.001)  # æ¨¡æ‹Ÿç¼“å­˜æ“ä½œå»¶è¿Ÿ

                        operation_time = (time.time() - operation_start) * 1000

                        return {
                            'op_id': op_id,
                            'success': True,
                            'latency_ms': operation_time
                        }

                    except Exception as e:
                        operation_time = (time.time() - operation_start) * 1000
                        return {
                            'op_id': op_id,
                            'success': False,
                            'latency_ms': operation_time,
                            'error': str(e)
                        }

                # æ‰§è¡Œè´Ÿè½½æµ‹è¯•
                load_start = time.time()
                tasks = [load_operation(i) for i in range(load_level)]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                load_duration = time.time() - load_start

                # åˆ†æè´Ÿè½½æµ‹è¯•ç»“æœ
                successful_ops = [r for r in results if isinstance(r, dict) and r.get('success')]
                failed_ops = [r for r in results if not isinstance(r, dict) or not r.get('success')]

                if successful_ops:
                    latencies = [r['latency_ms'] for r in successful_ops]
                    latencies.sort()

                    success_rate = len(successful_ops) / load_level
                    throughput = len(successful_ops) / load_duration

                    metrics.extend([
                        PerformanceMetric(
                            metric_name="load_test_success_rate",
                            timestamp=datetime.now(),
                            value=success_rate * 100,
                            unit="percent",
                            component="system",
                            test_scenario=f"load_{load_level}",
                            metadata={'concurrent_operations': load_level}
                        ),
                        PerformanceMetric(
                            metric_name="load_test_latency_p95",
                            timestamp=datetime.now(),
                            value=latencies[int(len(latencies) * 0.95)],
                            unit="ms",
                            component="system",
                            test_scenario=f"load_{load_level}"
                        ),
                        PerformanceMetric(
                            metric_name="load_test_throughput",
                            timestamp=datetime.now(),
                            value=throughput,
                            unit="ops/s",
                            component="system",
                            test_scenario=f"load_{load_level}"
                        )
                    ])

                    logger.info(f"Load: {load_level}, Success Rate: {success_rate:.2%}, "
                               f"P95 Latency: {latencies[int(len(latencies) * 0.95)]:.2f}ms, "
                               f"Throughput: {throughput:.2f} ops/s")

                    # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦è¾¾åˆ°æé™
                    if success_rate < 0.95 or latencies[int(len(latencies) * 0.95)] > 5000:
                        logger.info(f"System load limit reached at {load_level} concurrent operations")
                        break

                # çŸ­æš‚ä¼‘æ¯ä»¥è®©ç³»ç»Ÿæ¢å¤
                await asyncio.sleep(2)

            # åœæ­¢ç³»ç»Ÿç›‘æ§
            self.system_monitor.stop_monitoring()
            monitor_task.cancel()

            # è·å–ç³»ç»Ÿç›‘æ§æ‘˜è¦
            system_metrics = self.system_monitor.get_metrics_summary()

            # è®¡ç®—æ‘˜è¦ç»Ÿè®¡
            success_rates = [m.value for m in metrics if 'success_rate' in m.metric_name]
            throughputs = [m.value for m in metrics if 'throughput' in m.metric_name]

            summary_stats = {
                'max_concurrent_operations': max(load_levels[:len(success_rates)]) if success_rates else 0,
                'max_throughput_ops_per_sec': max(throughputs) if throughputs else 0,
                'avg_success_rate_percent': statistics.mean(success_rates) if success_rates else 0,
                'system_metrics': system_metrics,
                'performance_acceptable': min(success_rates) > 90 if success_rates else False
            }

            return BenchmarkResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                success=True,
                metrics=metrics,
                summary_stats=summary_stats
            )

        except Exception as e:
            logger.error(f"System load benchmark failed: {e}")
            self.system_monitor.stop_monitoring()

            return BenchmarkResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                success=False,
                metrics=metrics,
                summary_stats={},
                error_message=str(e)
            )

    def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("Generating comprehensive performance report...")

        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
        all_metrics = []
        for result in self.benchmark_results:
            all_metrics.extend(result.metrics)

        # æŒ‰ç»„ä»¶åˆ†ç»„æŒ‡æ ‡
        metrics_by_component = defaultdict(list)
        for metric in all_metrics:
            metrics_by_component[metric.component].append(metric)

        # ç”Ÿæˆç»„ä»¶çº§æ‘˜è¦
        component_summaries = {}
        for component, component_metrics in metrics_by_component.items():
            latency_metrics = [m for m in component_metrics if 'latency' in m.metric_name]
            throughput_metrics = [m for m in component_metrics if 'throughput' in m.metric_name]

            if latency_metrics:
                latencies = [m.value for m in latency_metrics]
                avg_latency = statistics.mean(latencies)
                p95_latency = sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0
            else:
                avg_latency = p95_latency = 0

            if throughput_metrics:
                throughputs = [m.value for m in throughput_metrics]
                avg_throughput = statistics.mean(throughputs)
                max_throughput = max(throughputs)
            else:
                avg_throughput = max_throughput = 0

            component_summaries[component] = {
                'total_metrics': len(component_metrics),
                'avg_latency_ms': avg_latency,
                'p95_latency_ms': p95_latency,
                'avg_throughput': avg_throughput,
                'max_throughput': max_throughput,
                'performance_grade': self._calculate_performance_grade(component, avg_latency, avg_throughput)
            }

        # è®¡ç®—æ€»ä½“æ€§èƒ½è¯„åˆ†
        successful_tests = [r for r in self.benchmark_results if r.success]
        overall_success_rate = len(successful_tests) / len(self.benchmark_results) if self.benchmark_results else 0

        overall_grade = statistics.mean([
            summary['performance_grade'] for summary in component_summaries.values()
        ]) if component_summaries else 0

        # æ€§èƒ½å»ºè®®
        recommendations = self._generate_performance_recommendations(component_summaries)

        report = {
            'report_generated_at': datetime.now().isoformat(),
            'test_configuration': {
                'vector_dimensions': self.config.vector_dimensions,
                'dataset_sizes': [self.config.small_dataset_size, self.config.medium_dataset_size, self.config.large_dataset_size],
                'concurrent_users': self.config.concurrent_users,
                'performance_thresholds': {
                    'vector_search_ms': self.config.vector_search_threshold_ms,
                    'ai_completion_ms': self.config.ai_completion_threshold_ms,
                    'cache_hit_ms': self.config.cache_hit_threshold_ms
                }
            },
            'overall_summary': {
                'total_benchmarks': len(self.benchmark_results),
                'successful_benchmarks': len(successful_tests),
                'success_rate': overall_success_rate,
                'overall_performance_grade': overall_grade,
                'total_metrics_collected': len(all_metrics)
            },
            'component_performance': component_summaries,
            'benchmark_results': [
                {
                    'test_name': r.test_name,
                    'duration_seconds': r.duration_seconds,
                    'success': r.success,
                    'summary_stats': r.summary_stats,
                    'error_message': r.error_message
                }
                for r in self.benchmark_results
            ],
            'performance_recommendations': recommendations,
            'detailed_metrics': [
                {
                    'metric_name': m.metric_name,
                    'value': m.value,
                    'unit': m.unit,
                    'component': m.component,
                    'test_scenario': m.test_scenario,
                    'timestamp': m.timestamp.isoformat()
                }
                for m in all_metrics[:100]  # é™åˆ¶è¯¦ç»†æŒ‡æ ‡æ•°é‡
            ]
        }

        return report

    def _calculate_performance_grade(self, component: str, avg_latency: float, avg_throughput: float) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ† (0-100)"""
        if component == "pgvector":
            latency_score = max(0, 100 - (avg_latency / self.config.vector_search_threshold_ms) * 50)
            throughput_score = min(100, (avg_throughput / self.config.throughput_threshold_qps) * 50)
        elif component == "ai_model":
            latency_score = max(0, 100 - (avg_latency / self.config.ai_completion_threshold_ms) * 50)
            throughput_score = min(100, avg_throughput * 10)  # è°ƒæ•´AIæ¨¡å‹ååé‡è¯„åˆ†
        elif component == "cache":
            latency_score = max(0, 100 - (avg_latency / self.config.cache_hit_threshold_ms) * 50)
            throughput_score = min(100, avg_throughput / 100)
        else:
            latency_score = throughput_score = 50  # é»˜è®¤ä¸­ç­‰è¯„åˆ†

        return (latency_score + throughput_score) / 2

    def _generate_performance_recommendations(self, component_summaries: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        for component, summary in component_summaries.items():
            grade = summary['performance_grade']
            avg_latency = summary['avg_latency_ms']

            if component == "pgvector":
                if grade < 70:
                    recommendations.append(f"ğŸ” pgvectoræ€§èƒ½å¯ä»¥ä¼˜åŒ–: è€ƒè™‘è°ƒæ•´ç´¢å¼•å‚æ•°æˆ–ä½¿ç”¨æ›´é«˜æ•ˆçš„ç´¢å¼•ç±»å‹")
                if avg_latency > self.config.vector_search_threshold_ms:
                    recommendations.append(f"âš¡ å‘é‡æœç´¢å»¶è¿Ÿè¾ƒé«˜ ({avg_latency:.2f}ms): å»ºè®®ä¼˜åŒ–æŸ¥è¯¢æˆ–å¢åŠ ç¡¬ä»¶èµ„æº")

            elif component == "ai_model":
                if grade < 70:
                    recommendations.append(f"ğŸ¤– AIæ¨¡å‹æ€§èƒ½éœ€è¦ä¼˜åŒ–: è€ƒè™‘è´Ÿè½½å‡è¡¡æˆ–æ¨¡å‹ç¼“å­˜ç­–ç•¥")
                if avg_latency > self.config.ai_completion_threshold_ms:
                    recommendations.append(f"ğŸ•’ AIå®Œæˆå»¶è¿Ÿè¾ƒé«˜ ({avg_latency:.2f}ms): å»ºè®®ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æˆ–ä¼˜åŒ–æç¤ºè¯")

            elif component == "cache":
                if grade < 70:
                    recommendations.append(f"ğŸ’¾ ç¼“å­˜æ€§èƒ½å¯ä»¥æ”¹å–„: æ£€æŸ¥Redisé…ç½®æˆ–ç½‘ç»œå»¶è¿Ÿ")
                if avg_latency > self.config.cache_hit_threshold_ms:
                    recommendations.append(f"ğŸš€ ç¼“å­˜è®¿é—®å»¶è¿Ÿè¾ƒé«˜ ({avg_latency:.2f}ms): è€ƒè™‘ç¼“å­˜é¢„çƒ­æˆ–å¢åŠ ç¼“å­˜å®¹é‡")

        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("âœ… ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ç›‘æ§å…³é”®æŒ‡æ ‡")

        return recommendations

    async def run_all_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("Starting comprehensive performance benchmark suite...")

        suite_start_time = time.time()

        # å®šä¹‰åŸºå‡†æµ‹è¯•åˆ—è¡¨
        benchmarks = [
            self.benchmark_vector_search_performance,
            self.benchmark_ai_model_performance,
            self.benchmark_cache_performance,
            self.benchmark_system_load_limits
        ]

        # é€ä¸ªæ‰§è¡ŒåŸºå‡†æµ‹è¯•
        for benchmark_func in benchmarks:
            try:
                logger.info(f"Running benchmark: {benchmark_func.__name__}")
                result = await benchmark_func()
                self.benchmark_results.append(result)

                status = 'PASSED' if result.success else 'FAILED'
                logger.info(f"Benchmark {result.test_name}: {status} ({result.duration_seconds:.2f}s)")

            except Exception as e:
                logger.error(f"Benchmark {benchmark_func.__name__} failed with exception: {e}")

                # åˆ›å»ºå¤±è´¥ç»“æœè®°å½•
                failed_result = BenchmarkResult(
                    test_name=benchmark_func.__name__,
                    start_time=datetime.now(),
                    end_time=datetime.now(),
                    duration_seconds=0,
                    success=False,
                    metrics=[],
                    summary_stats={},
                    error_message=str(e)
                )
                self.benchmark_results.append(failed_result)

        suite_duration = time.time() - suite_start_time

        # ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š
        performance_report = self.generate_performance_report()
        performance_report['benchmark_suite_duration_seconds'] = suite_duration

        logger.info(f"Performance benchmark suite completed in {suite_duration:.2f} seconds")

        return performance_report

    async def cleanup(self):
        """æ¸…ç†åŸºå‡†æµ‹è¯•ç¯å¢ƒ"""
        logger.info("Cleaning up performance benchmark environment...")

        # æ¸…ç†æµ‹è¯•æ•°æ®
        if self.db_pool:
            async with self.db_pool.acquire() as conn:
                await conn.execute("DELETE FROM benchmark_vectors")
                await conn.execute("DROP INDEX IF EXISTS idx_benchmark_hnsw")
                await conn.execute("DROP INDEX IF EXISTS idx_benchmark_ivfflat")

        # å…³é—­ç»„ä»¶
        if self.ai_manager:
            await self.ai_manager.shutdown()
        if self.cache_manager:
            await self.cache_manager.shutdown()
        if self.metrics_collector:
            await self.metrics_collector.shutdown()
        if self.db_pool:
            await self.db_pool.close()

async def main():
    """ä¸»åŸºå‡†æµ‹è¯•å…¥å£"""
    config = BenchmarkConfig()
    benchmark_suite = PerformanceBenchmarkSuite(config)

    try:
        await benchmark_suite.initialize()
        performance_report = await benchmark_suite.run_all_benchmarks()

        # ä¿å­˜æ€§èƒ½æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"/h/novellus/test_results/performance_benchmark_report_{timestamp}.json"

        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(performance_report, f, indent=2, ensure_ascii=False)

        print(f"\nPerformance benchmark report saved to: {report_file}")
        print(f"Overall performance grade: {performance_report['overall_summary']['overall_performance_grade']:.1f}/100")
        print(f"Success rate: {performance_report['overall_summary']['success_rate']:.2%}")

        # æ˜¾ç¤ºä¸»è¦æ€§èƒ½æŒ‡æ ‡
        print("\n=== Performance Summary ===")
        for component, summary in performance_report['component_performance'].items():
            print(f"{component.upper()}: Grade {summary['performance_grade']:.1f}/100, "
                  f"Avg Latency: {summary['avg_latency_ms']:.2f}ms, "
                  f"Max Throughput: {summary['max_throughput']:.2f}")

        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        print("\n=== Recommendations ===")
        for recommendation in performance_report['performance_recommendations']:
            print(f"  {recommendation}")

        return performance_report['overall_summary']['success_rate'] >= 0.8

    except Exception as e:
        logger.error(f"Performance benchmark suite failed: {e}")
        return False
    finally:
        await benchmark_suite.cleanup()

if __name__ == "__main__":
    asyncio.run(main())