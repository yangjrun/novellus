#!/usr/bin/env python3
"""
Automated Test Runner and Report Generator
è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œå™¨å’ŒæŠ¥å‘Šç”Ÿæˆå™¨ï¼Œç»Ÿä¸€ç®¡ç†å’Œæ‰§è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
"""

import asyncio
import json
import logging
import os
import sys
import time
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import argparse

import jinja2
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append('/h/novellus')
sys.path.append('/h/novellus/tests')

# å¯¼å…¥æµ‹è¯•å¥—ä»¶
from test_pgvector_suite import PgvectorTestSuite, TestConfig as PgVectorConfig
from test_ai_model_manager import AIModelManagerTestSuite, AITestConfig
from test_integration_suite import IntegrationTestSuite, IntegrationTestConfig
from performance_benchmark_suite import PerformanceBenchmarkSuite, BenchmarkConfig

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/h/novellus/test_results/test_runner.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TestSuiteType(str, Enum):
    """æµ‹è¯•å¥—ä»¶ç±»å‹"""
    PGVECTOR = "pgvector"
    AI_MODEL = "ai_model"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    ALL = "all"

@dataclass
class TestRunConfig:
    """æµ‹è¯•è¿è¡Œé…ç½®"""
    # æµ‹è¯•é€‰æ‹©
    test_suites: List[TestSuiteType] = field(default_factory=lambda: [TestSuiteType.ALL])
    skip_long_running_tests: bool = False
    parallel_execution: bool = True
    fail_fast: bool = False

    # ç¯å¢ƒé…ç½®
    environment: str = "test"
    cleanup_after_tests: bool = True
    generate_detailed_report: bool = True
    generate_charts: bool = True

    # æŠ¥å‘Šé…ç½®
    output_directory: str = "/h/novellus/test_results"
    report_format: List[str] = field(default_factory=lambda: ["json", "html", "markdown"])
    include_raw_data: bool = False

    # é€šçŸ¥é…ç½®
    enable_notifications: bool = False
    notification_webhook: Optional[str] = None

    # æ•°æ®åº“é…ç½®
    db_host: str = "localhost"
    db_port: int = 5432
    db_name: str = "postgres"
    db_user: str = "postgres"
    db_password: str = "postgres"
    redis_url: str = "redis://localhost:6379"

@dataclass
class TestSuiteResult:
    """æµ‹è¯•å¥—ä»¶ç»“æœ"""
    suite_name: str
    suite_type: TestSuiteType
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    success: bool
    tests_run: int
    tests_passed: int
    tests_failed: int
    success_rate: float
    error_message: str = ""
    detailed_results: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)

@dataclass
class TestRunSummary:
    """æµ‹è¯•è¿è¡Œæ€»ç»“"""
    run_id: str
    start_time: datetime
    end_time: datetime
    total_duration_seconds: float
    environment: str
    configuration: TestRunConfig
    suite_results: List[TestSuiteResult]
    overall_success: bool
    overall_success_rate: float
    total_tests: int
    total_passed: int
    total_failed: int
    critical_issues: List[str] = field(default_factory=list)
    performance_summary: Dict[str, Any] = field(default_factory=dict)

class AutomatedTestRunner:
    """è‡ªåŠ¨åŒ–æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config: TestRunConfig = None):
        self.config = config or TestRunConfig()
        self.run_id = f"test_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.output_dir = Path(self.config.output_directory) / self.run_id

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # é…ç½®æ–‡ä»¶è·¯å¾„
        self.config_file = self.output_dir / "test_config.json"
        self.log_file = self.output_dir / "test_execution.log"

        # æµ‹è¯•ç»“æœ
        self.suite_results: List[TestSuiteResult] = []
        self.test_summary: Optional[TestRunSummary] = None

        # è®¾ç½®æ—¥å¿—
        self._setup_logging()

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(self.log_file)
        file_handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)

        # æ·»åŠ åˆ°æ ¹æ—¥å¿—å™¨
        root_logger = logging.getLogger()
        root_logger.addHandler(file_handler)

    async def run_all_tests(self) -> TestRunSummary:
        """è¿è¡Œæ‰€æœ‰é€‰å®šçš„æµ‹è¯•å¥—ä»¶"""
        logger.info(f"Starting automated test run {self.run_id}")
        run_start_time = datetime.now()

        # ä¿å­˜é…ç½®
        self._save_test_config()

        # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•å¥—ä»¶
        test_suites_to_run = self._determine_test_suites()

        logger.info(f"Will run {len(test_suites_to_run)} test suites: {[s.value for s in test_suites_to_run]}")

        # è¿è¡Œæµ‹è¯•å¥—ä»¶
        if self.config.parallel_execution and len(test_suites_to_run) > 1:
            await self._run_tests_parallel(test_suites_to_run)
        else:
            await self._run_tests_sequential(test_suites_to_run)

        run_end_time = datetime.now()

        # ç”Ÿæˆæµ‹è¯•æ€»ç»“
        self.test_summary = self._generate_test_summary(run_start_time, run_end_time)

        # ç”ŸæˆæŠ¥å‘Š
        await self._generate_reports()

        # æ¸…ç†èµ„æº
        if self.config.cleanup_after_tests:
            await self._cleanup_test_resources()

        # å‘é€é€šçŸ¥
        if self.config.enable_notifications:
            await self._send_notifications()

        logger.info(f"Test run {self.run_id} completed. Overall success rate: {self.test_summary.overall_success_rate:.2%}")

        return self.test_summary

    def _determine_test_suites(self) -> List[TestSuiteType]:
        """ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•å¥—ä»¶"""
        if TestSuiteType.ALL in self.config.test_suites:
            return [TestSuiteType.PGVECTOR, TestSuiteType.AI_MODEL, TestSuiteType.INTEGRATION, TestSuiteType.PERFORMANCE]
        else:
            return [suite for suite in self.config.test_suites if suite != TestSuiteType.ALL]

    async def _run_tests_sequential(self, test_suites: List[TestSuiteType]):
        """é¡ºåºè¿è¡Œæµ‹è¯•å¥—ä»¶"""
        for suite_type in test_suites:
            try:
                logger.info(f"Running test suite: {suite_type.value}")
                result = await self._run_single_test_suite(suite_type)
                self.suite_results.append(result)

                if self.config.fail_fast and not result.success:
                    logger.error(f"Test suite {suite_type.value} failed, stopping execution (fail_fast=True)")
                    break

            except Exception as e:
                logger.error(f"Failed to run test suite {suite_type.value}: {e}")
                logger.error(traceback.format_exc())

                # åˆ›å»ºå¤±è´¥ç»“æœ
                failed_result = TestSuiteResult(
                    suite_name=suite_type.value,
                    suite_type=suite_type,
                    start_time=datetime.now(),
                    end_time=datetime.now(),
                    duration_seconds=0,
                    success=False,
                    tests_run=0,
                    tests_passed=0,
                    tests_failed=1,
                    success_rate=0.0,
                    error_message=str(e)
                )
                self.suite_results.append(failed_result)

                if self.config.fail_fast:
                    break

    async def _run_tests_parallel(self, test_suites: List[TestSuiteType]):
        """å¹¶è¡Œè¿è¡Œæµ‹è¯•å¥—ä»¶"""
        logger.info("Running test suites in parallel...")

        async def run_suite_with_error_handling(suite_type: TestSuiteType):
            try:
                return await self._run_single_test_suite(suite_type)
            except Exception as e:
                logger.error(f"Test suite {suite_type.value} failed: {e}")
                return TestSuiteResult(
                    suite_name=suite_type.value,
                    suite_type=suite_type,
                    start_time=datetime.now(),
                    end_time=datetime.now(),
                    duration_seconds=0,
                    success=False,
                    tests_run=0,
                    tests_passed=0,
                    tests_failed=1,
                    success_rate=0.0,
                    error_message=str(e)
                )

        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        tasks = [run_suite_with_error_handling(suite_type) for suite_type in test_suites]

        # æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, TestSuiteResult):
                self.suite_results.append(result)
            else:
                logger.error(f"Unexpected result type: {type(result)}")

    async def _run_single_test_suite(self, suite_type: TestSuiteType) -> TestSuiteResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶"""
        start_time = datetime.now()

        try:
            if suite_type == TestSuiteType.PGVECTOR:
                return await self._run_pgvector_tests()
            elif suite_type == TestSuiteType.AI_MODEL:
                return await self._run_ai_model_tests()
            elif suite_type == TestSuiteType.INTEGRATION:
                return await self._run_integration_tests()
            elif suite_type == TestSuiteType.PERFORMANCE:
                return await self._run_performance_tests()
            else:
                raise ValueError(f"Unknown test suite type: {suite_type}")

        except Exception as e:
            end_time = datetime.now()
            logger.error(f"Test suite {suite_type.value} execution failed: {e}")

            return TestSuiteResult(
                suite_name=suite_type.value,
                suite_type=suite_type,
                start_time=start_time,
                end_time=end_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                success=False,
                tests_run=0,
                tests_passed=0,
                tests_failed=1,
                success_rate=0.0,
                error_message=str(e)
            )

    async def _run_pgvector_tests(self) -> TestSuiteResult:
        """è¿è¡Œpgvectoræµ‹è¯•å¥—ä»¶"""
        start_time = datetime.now()

        config = PgVectorConfig(
            db_host=self.config.db_host,
            db_port=self.config.db_port,
            db_name=self.config.db_name,
            db_user=self.config.db_user,
            db_password=self.config.db_password
        )

        test_suite = PgvectorTestSuite(config)

        try:
            await test_suite.initialize()
            results = await test_suite.run_all_tests()

            # è§£æç»“æœ
            total_tests = results['total_tests']
            passed_tests = results['passed_tests']
            success_rate = results['success_rate']

            end_time = datetime.now()

            return TestSuiteResult(
                suite_name="pgvector_comprehensive",
                suite_type=TestSuiteType.PGVECTOR,
                start_time=start_time,
                end_time=end_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                success=success_rate == 1.0,
                tests_run=total_tests,
                tests_passed=passed_tests,
                tests_failed=total_tests - passed_tests,
                success_rate=success_rate,
                detailed_results=results
            )

        finally:
            await test_suite.cleanup()

    async def _run_ai_model_tests(self) -> TestSuiteResult:
        """è¿è¡ŒAIæ¨¡å‹æµ‹è¯•å¥—ä»¶"""
        start_time = datetime.now()

        config = AITestConfig(
            test_db_url=f"postgresql+asyncpg://{self.config.db_user}:{self.config.db_password}@{self.config.db_host}:{self.config.db_port}/{self.config.db_name}",
            test_redis_url=self.config.redis_url,
            mock_api_responses=True  # åœ¨è‡ªåŠ¨åŒ–æµ‹è¯•ä¸­ä½¿ç”¨æ¨¡æ‹Ÿ
        )

        test_suite = AIModelManagerTestSuite(config)

        try:
            await test_suite.initialize()
            results = await test_suite.run_all_tests()

            # è§£æç»“æœ
            total_tests = results['total_tests']
            passed_tests = results['passed_tests']
            success_rate = results['success_rate']

            end_time = datetime.now()

            return TestSuiteResult(
                suite_name="ai_model_manager_comprehensive",
                suite_type=TestSuiteType.AI_MODEL,
                start_time=start_time,
                end_time=end_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                success=success_rate >= 0.8,  # AIæµ‹è¯•å…è®¸ä¸€å®šå®¹é”™
                tests_run=total_tests,
                tests_passed=passed_tests,
                tests_failed=total_tests - passed_tests,
                success_rate=success_rate,
                detailed_results=results
            )

        finally:
            await test_suite.cleanup()

    async def _run_integration_tests(self) -> TestSuiteResult:
        """è¿è¡Œé›†æˆæµ‹è¯•å¥—ä»¶"""
        start_time = datetime.now()

        config = IntegrationTestConfig(
            db_host=self.config.db_host,
            db_port=self.config.db_port,
            db_name=self.config.db_name,
            db_user=self.config.db_user,
            db_password=self.config.db_password,
            redis_url=self.config.redis_url
        )

        test_suite = IntegrationTestSuite(config)

        try:
            await test_suite.initialize()
            results = await test_suite.run_all_tests()

            # è§£æç»“æœ
            total_tests = results['total_tests']
            passed_tests = results['passed_tests']
            success_rate = results['success_rate']

            end_time = datetime.now()

            return TestSuiteResult(
                suite_name="comprehensive_integration",
                suite_type=TestSuiteType.INTEGRATION,
                start_time=start_time,
                end_time=end_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                success=success_rate >= 0.8,  # é›†æˆæµ‹è¯•å…è®¸ä¸€å®šå®¹é”™
                tests_run=total_tests,
                tests_passed=passed_tests,
                tests_failed=total_tests - passed_tests,
                success_rate=success_rate,
                detailed_results=results
            )

        finally:
            await test_suite.cleanup()

    async def _run_performance_tests(self) -> TestSuiteResult:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
        start_time = datetime.now()

        # å¦‚æœè·³è¿‡é•¿æ—¶é—´è¿è¡Œçš„æµ‹è¯•ï¼Œå‡å°‘æ€§èƒ½æµ‹è¯•è§„æ¨¡
        config = BenchmarkConfig(
            db_host=self.config.db_host,
            db_port=self.config.db_port,
            db_name=self.config.db_name,
            db_user=self.config.db_user,
            db_password=self.config.db_password,
            redis_url=self.config.redis_url
        )

        if self.config.skip_long_running_tests:
            config.small_dataset_size = 100
            config.medium_dataset_size = 1000
            config.large_dataset_size = 5000
            config.concurrent_users = [1, 5, 10]

        test_suite = PerformanceBenchmarkSuite(config)

        try:
            await test_suite.initialize()
            results = await test_suite.run_all_benchmarks()

            # è§£æç»“æœ
            overall_summary = results['overall_summary']
            total_tests = overall_summary['total_benchmarks']
            passed_tests = overall_summary['successful_benchmarks']
            success_rate = overall_summary['success_rate']

            end_time = datetime.now()

            return TestSuiteResult(
                suite_name="performance_comprehensive",
                suite_type=TestSuiteType.PERFORMANCE,
                start_time=start_time,
                end_time=end_time,
                duration_seconds=(end_time - start_time).total_seconds(),
                success=success_rate >= 0.75,  # æ€§èƒ½æµ‹è¯•å…è®¸æ›´å¤šå®¹é”™
                tests_run=total_tests,
                tests_passed=passed_tests,
                tests_failed=total_tests - passed_tests,
                success_rate=success_rate,
                detailed_results=results,
                performance_metrics=overall_summary
            )

        finally:
            await test_suite.cleanup()

    def _generate_test_summary(self, start_time: datetime, end_time: datetime) -> TestRunSummary:
        """ç”Ÿæˆæµ‹è¯•è¿è¡Œæ€»ç»“"""
        total_tests = sum(result.tests_run for result in self.suite_results)
        total_passed = sum(result.tests_passed for result in self.suite_results)
        total_failed = sum(result.tests_failed for result in self.suite_results)

        overall_success_rate = total_passed / total_tests if total_tests > 0 else 0
        overall_success = all(result.success for result in self.suite_results)

        # è¯†åˆ«å…³é”®é—®é¢˜
        critical_issues = []
        for result in self.suite_results:
            if not result.success:
                critical_issues.append(f"{result.suite_name}: {result.error_message}")
            elif result.success_rate < 0.8:
                critical_issues.append(f"{result.suite_name}: Low success rate ({result.success_rate:.2%})")

        # æ€§èƒ½æ‘˜è¦
        performance_summary = {}
        perf_result = next((r for r in self.suite_results if r.suite_type == TestSuiteType.PERFORMANCE), None)
        if perf_result and perf_result.performance_metrics:
            performance_summary = {
                'overall_grade': perf_result.performance_metrics.get('overall_performance_grade', 0),
                'component_grades': {},
                'recommendations': []
            }

        return TestRunSummary(
            run_id=self.run_id,
            start_time=start_time,
            end_time=end_time,
            total_duration_seconds=(end_time - start_time).total_seconds(),
            environment=self.config.environment,
            configuration=self.config,
            suite_results=self.suite_results,
            overall_success=overall_success,
            overall_success_rate=overall_success_rate,
            total_tests=total_tests,
            total_passed=total_passed,
            total_failed=total_failed,
            critical_issues=critical_issues,
            performance_summary=performance_summary
        )

    async def _generate_reports(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("Generating test reports...")

        # JSONæŠ¥å‘Š
        if "json" in self.config.report_format:
            await self._generate_json_report()

        # HTMLæŠ¥å‘Š
        if "html" in self.config.report_format:
            await self._generate_html_report()

        # MarkdownæŠ¥å‘Š
        if "markdown" in self.config.report_format:
            await self._generate_markdown_report()

        # å›¾è¡¨æŠ¥å‘Š
        if self.config.generate_charts:
            await self._generate_charts()

    async def _generate_json_report(self):
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        json_file = self.output_dir / "test_report.json"

        report_data = {
            'test_run_summary': {
                'run_id': self.test_summary.run_id,
                'start_time': self.test_summary.start_time.isoformat(),
                'end_time': self.test_summary.end_time.isoformat(),
                'duration_seconds': self.test_summary.total_duration_seconds,
                'environment': self.test_summary.environment,
                'overall_success': self.test_summary.overall_success,
                'overall_success_rate': self.test_summary.overall_success_rate,
                'total_tests': self.test_summary.total_tests,
                'total_passed': self.test_summary.total_passed,
                'total_failed': self.test_summary.total_failed,
                'critical_issues': self.test_summary.critical_issues
            },
            'suite_results': [
                {
                    'suite_name': result.suite_name,
                    'suite_type': result.suite_type.value,
                    'start_time': result.start_time.isoformat(),
                    'end_time': result.end_time.isoformat(),
                    'duration_seconds': result.duration_seconds,
                    'success': result.success,
                    'tests_run': result.tests_run,
                    'tests_passed': result.tests_passed,
                    'tests_failed': result.tests_failed,
                    'success_rate': result.success_rate,
                    'error_message': result.error_message,
                    'detailed_results': result.detailed_results if self.config.include_raw_data else {}
                }
                for result in self.suite_results
            ],
            'configuration': {
                'test_suites': [suite.value for suite in self.config.test_suites],
                'parallel_execution': self.config.parallel_execution,
                'skip_long_running_tests': self.config.skip_long_running_tests,
                'environment': self.config.environment
            }
        }

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        logger.info(f"JSON report saved to: {json_file}")

    async def _generate_html_report(self):
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        html_file = self.output_dir / "test_report.html"

        # HTMLæ¨¡æ¿
        html_template = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Novellus Test Report - {{ summary.run_id }}</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; margin: -20px -20px 20px; border-radius: 8px 8px 0 0; }
        .summary { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }
        .metric-card { background: #f8f9fa; padding: 15px; border-radius: 6px; text-align: center; border: 1px solid #e9ecef; }
        .metric-value { font-size: 2em; font-weight: bold; margin-bottom: 5px; }
        .metric-label { color: #6c757d; font-size: 0.9em; }
        .success { color: #28a745; }
        .warning { color: #ffc107; }
        .danger { color: #dc3545; }
        .suite-results { margin: 20px 0; }
        .suite-card { background: white; border: 1px solid #ddd; border-radius: 6px; margin: 10px 0; overflow: hidden; }
        .suite-header { padding: 15px; background: #f8f9fa; border-bottom: 1px solid #ddd; display: flex; justify-content: space-between; align-items: center; }
        .suite-content { padding: 15px; }
        .status-badge { padding: 5px 10px; border-radius: 15px; color: white; font-size: 0.8em; font-weight: bold; }
        .status-success { background-color: #28a745; }
        .status-failure { background-color: #dc3545; }
        .progress-bar { width: 100%; height: 20px; background-color: #e9ecef; border-radius: 10px; overflow: hidden; margin: 10px 0; }
        .progress-fill { height: 100%; background: linear-gradient(90deg, #28a745, #20c997); transition: width 0.3s ease; }
        .issues-section { margin: 20px 0; padding: 15px; background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 6px; }
        .timestamp { color: #6c757d; font-size: 0.9em; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ§ª Novellus Test Report</h1>
            <h2>{{ summary.run_id }}</h2>
            <p class="timestamp">Generated on {{ summary.end_time.strftime('%Y-%m-%d %H:%M:%S') }}</p>
        </div>

        <div class="summary">
            <div class="metric-card">
                <div class="metric-value {% if summary.overall_success %}success{% else %}danger{% endif %}">
                    {% if summary.overall_success %}âœ…{% else %}âŒ{% endif %}
                </div>
                <div class="metric-label">Overall Status</div>
            </div>
            <div class="metric-card">
                <div class="metric-value {{ 'success' if summary.overall_success_rate >= 0.9 else 'warning' if summary.overall_success_rate >= 0.7 else 'danger' }}">
                    {{ "%.1f"|format(summary.overall_success_rate * 100) }}%
                </div>
                <div class="metric-label">Success Rate</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ summary.total_tests }}</div>
                <div class="metric-label">Total Tests</div>
            </div>
            <div class="metric-card">
                <div class="metric-value success">{{ summary.total_passed }}</div>
                <div class="metric-label">Passed</div>
            </div>
            <div class="metric-card">
                <div class="metric-value danger">{{ summary.total_failed }}</div>
                <div class="metric-label">Failed</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{{ "%.1f"|format(summary.total_duration_seconds) }}s</div>
                <div class="metric-label">Duration</div>
            </div>
        </div>

        {% if summary.critical_issues %}
        <div class="issues-section">
            <h3>ğŸš¨ Critical Issues</h3>
            <ul>
                {% for issue in summary.critical_issues %}
                <li>{{ issue }}</li>
                {% endfor %}
            </ul>
        </div>
        {% endif %}

        <div class="suite-results">
            <h3>ğŸ“Š Test Suite Results</h3>
            {% for result in suite_results %}
            <div class="suite-card">
                <div class="suite-header">
                    <div>
                        <h4>{{ result.suite_name }}</h4>
                        <small>{{ result.suite_type.value }} | {{ "%.1f"|format(result.duration_seconds) }}s</small>
                    </div>
                    <span class="status-badge {% if result.success %}status-success{% else %}status-failure{% endif %}">
                        {% if result.success %}PASSED{% else %}FAILED{% endif %}
                    </span>
                </div>
                <div class="suite-content">
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {{ result.success_rate * 100 }}%"></div>
                    </div>
                    <p>
                        <strong>{{ result.tests_passed }}/{{ result.tests_run }}</strong> tests passed
                        ({{ "%.1f"|format(result.success_rate * 100) }}%)
                    </p>
                    {% if result.error_message %}
                    <p><strong>Error:</strong> {{ result.error_message }}</p>
                    {% endif %}
                </div>
            </div>
            {% endfor %}
        </div>
    </div>
</body>
</html>
        """

        # æ¸²æŸ“æ¨¡æ¿
        template = jinja2.Template(html_template)
        html_content = template.render(
            summary=self.test_summary,
            suite_results=self.suite_results
        )

        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        logger.info(f"HTML report saved to: {html_file}")

    async def _generate_markdown_report(self):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        md_file = self.output_dir / "test_report.md"

        # ç”ŸæˆMarkdownå†…å®¹
        md_content = f"""# ğŸ§ª Novellus Test Report

**Run ID:** {self.test_summary.run_id}
**Environment:** {self.test_summary.environment}
**Generated:** {self.test_summary.end_time.strftime('%Y-%m-%d %H:%M:%S')}
**Duration:** {self.test_summary.total_duration_seconds:.1f} seconds

## ğŸ“Š Summary

| Metric | Value |
|--------|-------|
| Overall Status | {'âœ… PASSED' if self.test_summary.overall_success else 'âŒ FAILED'} |
| Success Rate | {self.test_summary.overall_success_rate:.1%} |
| Total Tests | {self.test_summary.total_tests} |
| Passed Tests | {self.test_summary.total_passed} |
| Failed Tests | {self.test_summary.total_failed} |

"""

        # æ·»åŠ å…³é”®é—®é¢˜
        if self.test_summary.critical_issues:
            md_content += "## ğŸš¨ Critical Issues\n\n"
            for issue in self.test_summary.critical_issues:
                md_content += f"- {issue}\n"
            md_content += "\n"

        # æ·»åŠ æµ‹è¯•å¥—ä»¶ç»“æœ
        md_content += "## ğŸ“‹ Test Suite Results\n\n"

        for result in self.suite_results:
            status = "âœ… PASSED" if result.success else "âŒ FAILED"
            md_content += f"### {result.suite_name}\n\n"
            md_content += f"- **Status:** {status}\n"
            md_content += f"- **Type:** {result.suite_type.value}\n"
            md_content += f"- **Duration:** {result.duration_seconds:.1f} seconds\n"
            md_content += f"- **Tests:** {result.tests_passed}/{result.tests_run} passed ({result.success_rate:.1%})\n"

            if result.error_message:
                md_content += f"- **Error:** {result.error_message}\n"

            md_content += "\n"

        # æ·»åŠ é…ç½®ä¿¡æ¯
        md_content += "## âš™ï¸ Configuration\n\n"
        md_content += f"- **Test Suites:** {', '.join([suite.value for suite in self.config.test_suites])}\n"
        md_content += f"- **Parallel Execution:** {'Yes' if self.config.parallel_execution else 'No'}\n"
        md_content += f"- **Skip Long Tests:** {'Yes' if self.config.skip_long_running_tests else 'No'}\n"
        md_content += f"- **Environment:** {self.config.environment}\n"

        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

        logger.info(f"Markdown report saved to: {md_file}")

    async def _generate_charts(self):
        """ç”Ÿæˆå›¾è¡¨"""
        if not self.config.generate_charts:
            return

        try:
            # è®¾ç½®matplotlibæ ·å¼
            plt.style.use('seaborn-v0_8')
            sns.set_palette("husl")

            # åˆ›å»ºå›¾è¡¨ç›®å½•
            charts_dir = self.output_dir / "charts"
            charts_dir.mkdir(exist_ok=True)

            # 1. æµ‹è¯•å¥—ä»¶æˆåŠŸç‡å›¾è¡¨
            await self._generate_success_rate_chart(charts_dir)

            # 2. æµ‹è¯•æŒç»­æ—¶é—´å›¾è¡¨
            await self._generate_duration_chart(charts_dir)

            # 3. æµ‹è¯•ç»“æœåˆ†å¸ƒå›¾è¡¨
            await self._generate_distribution_chart(charts_dir)

            logger.info(f"Charts saved to: {charts_dir}")

        except Exception as e:
            logger.warning(f"Failed to generate charts: {e}")

    async def _generate_success_rate_chart(self, charts_dir: Path):
        """ç”ŸæˆæˆåŠŸç‡å›¾è¡¨"""
        # å‡†å¤‡æ•°æ®
        suite_names = [result.suite_name for result in self.suite_results]
        success_rates = [result.success_rate * 100 for result in self.suite_results]

        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(10, 6))
        bars = plt.bar(suite_names, success_rates)

        # è®¾ç½®é¢œè‰²
        for bar, rate in zip(bars, success_rates):
            if rate >= 90:
                bar.set_color('#28a745')  # ç»¿è‰²
            elif rate >= 70:
                bar.set_color('#ffc107')  # é»„è‰²
            else:
                bar.set_color('#dc3545')  # çº¢è‰²

        plt.title('Test Suite Success Rates', fontsize=16, fontweight='bold')
        plt.xlabel('Test Suite')
        plt.ylabel('Success Rate (%)')
        plt.xticks(rotation=45, ha='right')
        plt.ylim(0, 100)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars, success_rates):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{rate:.1f}%', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(charts_dir / 'success_rates.png', dpi=300, bbox_inches='tight')
        plt.close()

    async def _generate_duration_chart(self, charts_dir: Path):
        """ç”ŸæˆæŒç»­æ—¶é—´å›¾è¡¨"""
        # å‡†å¤‡æ•°æ®
        suite_names = [result.suite_name for result in self.suite_results]
        durations = [result.duration_seconds for result in self.suite_results]

        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(10, 6))
        bars = plt.bar(suite_names, durations, color='skyblue')

        plt.title('Test Suite Execution Duration', fontsize=16, fontweight='bold')
        plt.xlabel('Test Suite')
        plt.ylabel('Duration (seconds)')
        plt.xticks(rotation=45, ha='right')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, duration in zip(bars, durations):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(durations)*0.01,
                    f'{duration:.1f}s', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(charts_dir / 'durations.png', dpi=300, bbox_inches='tight')
        plt.close()

    async def _generate_distribution_chart(self, charts_dir: Path):
        """ç”Ÿæˆæµ‹è¯•ç»“æœåˆ†å¸ƒå›¾è¡¨"""
        # å‡†å¤‡æ•°æ®
        total_passed = sum(result.tests_passed for result in self.suite_results)
        total_failed = sum(result.tests_failed for result in self.suite_results)

        # åˆ›å»ºé¥¼å›¾
        plt.figure(figsize=(8, 8))
        labels = ['Passed', 'Failed']
        sizes = [total_passed, total_failed]
        colors = ['#28a745', '#dc3545']

        # åªæ˜¾ç¤ºéé›¶çš„éƒ¨åˆ†
        non_zero_labels = []
        non_zero_sizes = []
        non_zero_colors = []

        for label, size, color in zip(labels, sizes, colors):
            if size > 0:
                non_zero_labels.append(f'{label}\n({size} tests)')
                non_zero_sizes.append(size)
                non_zero_colors.append(color)

        if non_zero_sizes:
            plt.pie(non_zero_sizes, labels=non_zero_labels, colors=non_zero_colors,
                   autopct='%1.1f%%', startangle=90)
            plt.title('Overall Test Results Distribution', fontsize=16, fontweight='bold')
        else:
            plt.text(0.5, 0.5, 'No Test Data Available', ha='center', va='center',
                    transform=plt.gca().transAxes, fontsize=16)

        plt.axis('equal')
        plt.tight_layout()
        plt.savefig(charts_dir / 'distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _save_test_config(self):
        """ä¿å­˜æµ‹è¯•é…ç½®"""
        config_data = {
            'run_id': self.run_id,
            'test_suites': [suite.value for suite in self.config.test_suites],
            'skip_long_running_tests': self.config.skip_long_running_tests,
            'parallel_execution': self.config.parallel_execution,
            'fail_fast': self.config.fail_fast,
            'environment': self.config.environment,
            'cleanup_after_tests': self.config.cleanup_after_tests,
            'generate_detailed_report': self.config.generate_detailed_report,
            'generate_charts': self.config.generate_charts,
            'output_directory': str(self.output_dir),
            'report_format': self.config.report_format,
            'database_config': {
                'host': self.config.db_host,
                'port': self.config.db_port,
                'database': self.config.db_name
            }
        }

        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)

    async def _cleanup_test_resources(self):
        """æ¸…ç†æµ‹è¯•èµ„æº"""
        logger.info("Cleaning up test resources...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘ï¼Œæ¯”å¦‚åˆ é™¤ä¸´æ—¶æ•°æ®ç­‰

    async def _send_notifications(self):
        """å‘é€æµ‹è¯•ç»“æœé€šçŸ¥"""
        if not self.config.notification_webhook:
            return

        try:
            # æ„å»ºé€šçŸ¥æ¶ˆæ¯
            message = {
                'text': f"Test Run {self.run_id} Completed",
                'attachments': [
                    {
                        'color': 'good' if self.test_summary.overall_success else 'danger',
                        'fields': [
                            {
                                'title': 'Overall Status',
                                'value': 'PASSED' if self.test_summary.overall_success else 'FAILED',
                                'short': True
                            },
                            {
                                'title': 'Success Rate',
                                'value': f"{self.test_summary.overall_success_rate:.1%}",
                                'short': True
                            },
                            {
                                'title': 'Total Tests',
                                'value': str(self.test_summary.total_tests),
                                'short': True
                            },
                            {
                                'title': 'Duration',
                                'value': f"{self.test_summary.total_duration_seconds:.1f}s",
                                'short': True
                            }
                        ]
                    }
                ]
            }

            # å‘é€é€šçŸ¥ (è¿™é‡Œå¯ä»¥å®ç°å®é™…çš„Webhookè°ƒç”¨)
            logger.info(f"Notification would be sent to: {self.config.notification_webhook}")

        except Exception as e:
            logger.warning(f"Failed to send notification: {e}")

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Novellus Automated Test Runner')

    parser.add_argument('--suites', nargs='+', choices=['pgvector', 'ai_model', 'integration', 'performance', 'all'],
                       default=['all'], help='Test suites to run')
    parser.add_argument('--environment', default='test', help='Test environment')
    parser.add_argument('--parallel', action='store_true', help='Run test suites in parallel')
    parser.add_argument('--fail-fast', action='store_true', help='Stop on first failure')
    parser.add_argument('--skip-long', action='store_true', help='Skip long-running tests')
    parser.add_argument('--no-cleanup', action='store_true', help='Skip cleanup after tests')
    parser.add_argument('--no-charts', action='store_true', help='Skip chart generation')
    parser.add_argument('--output-dir', default='/h/novellus/test_results', help='Output directory')
    parser.add_argument('--formats', nargs='+', choices=['json', 'html', 'markdown'],
                       default=['json', 'html'], help='Report formats')

    # æ•°æ®åº“é…ç½®
    parser.add_argument('--db-host', default='localhost', help='Database host')
    parser.add_argument('--db-port', type=int, default=5432, help='Database port')
    parser.add_argument('--db-name', default='postgres', help='Database name')
    parser.add_argument('--db-user', default='postgres', help='Database user')
    parser.add_argument('--db-password', default='postgres', help='Database password')
    parser.add_argument('--redis-url', default='redis://localhost:6379', help='Redis URL')

    return parser.parse_args()

async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()

    # æ„å»ºé…ç½®
    config = TestRunConfig(
        test_suites=[TestSuiteType(suite) for suite in args.suites],
        skip_long_running_tests=args.skip_long,
        parallel_execution=args.parallel,
        fail_fast=args.fail_fast,
        environment=args.environment,
        cleanup_after_tests=not args.no_cleanup,
        generate_charts=not args.no_charts,
        output_directory=args.output_dir,
        report_format=args.formats,
        db_host=args.db_host,
        db_port=args.db_port,
        db_name=args.db_name,
        db_user=args.db_user,
        db_password=args.db_password,
        redis_url=args.redis_url
    )

    # åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•
    test_runner = AutomatedTestRunner(config)

    try:
        test_summary = await test_runner.run_all_tests()

        # è¾“å‡ºç»“æœæ‘˜è¦
        print(f"\n{'='*60}")
        print(f"ğŸ§ª Test Run {test_summary.run_id} Completed")
        print(f"{'='*60}")
        print(f"Overall Status: {'âœ… PASSED' if test_summary.overall_success else 'âŒ FAILED'}")
        print(f"Success Rate:   {test_summary.overall_success_rate:.1%}")
        print(f"Total Tests:    {test_summary.total_tests}")
        print(f"Passed:         {test_summary.total_passed}")
        print(f"Failed:         {test_summary.total_failed}")
        print(f"Duration:       {test_summary.total_duration_seconds:.1f} seconds")
        print(f"Reports:        {test_runner.output_dir}")

        if test_summary.critical_issues:
            print(f"\nğŸš¨ Critical Issues:")
            for issue in test_summary.critical_issues:
                print(f"  - {issue}")

        return 0 if test_summary.overall_success else 1

    except Exception as e:
        logger.error(f"Test runner failed: {e}")
        logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main()))